# create_vector_dbs.py
"""
Create vector databases from your existing data
Run this BEFORE running the Streamlit app
"""

import os
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from pathlib import Path
import json
import sqlite3
import pandas as pd
from tqdm import tqdm


def create_vector_database_from_sqlite(db_path, output_dir, db_name):
    """Create vector database from SQLite database"""

    print(f"\nüì¶ Creating vector database: {db_name}")

    # Create output directory
    output_dir = Path(output_dir) / db_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load sentence transformer model
    print("   Loading AI model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Connect to SQLite database
    conn = sqlite3.connect(db_path)

    # Get all documents
    print("   Loading documents...")

    # Try different table names
    possible_tables = [
        'all_documents',
        'documents',
        'docs',
        'texts',
        'content'
    ]

    table_name = None
    for table in possible_tables:
        try:
            cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            if count > 0:
                table_name = table
                print(f"   Found table '{table}' with {count} documents")
                break
        except:
            continue

    if not table_name:
        # List all tables
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
        all_tables = [row[0] for row in cursor.fetchall()]
        print(f"   Available tables: {all_tables}")

        # Try to find any table with content
        for table in all_tables:
            try:
                cursor = conn.execute(f"SELECT * FROM {table} LIMIT 1")
                columns = [description[0] for description in cursor.description]
                if 'content' in columns or 'text' in columns:
                    table_name = table
                    print(f"   Using table '{table}'")
                    break
            except:
                continue

    if not table_name:
        print(f"   ‚ùå No suitable table found in {db_path}")
        return None

    # Load data
    query = f"""
    SELECT 
        COALESCE(title, 'Untitled') as title,
        COALESCE(content, '') as content,
        COALESCE(author, 'Unknown') as author,
        COALESCE(source_type, 'Unknown') as source_type,
        COALESCE(url, '') as url,
        COALESCE(word_count, 0) as word_count,
        COALESCE(import_date, '') as import_date
    FROM {table_name}
    WHERE content IS NOT NULL AND content != ''
    """

    df = pd.read_sql_query(query, conn)
    conn.close()

    if len(df) == 0:
        print(f"   ‚ùå No documents with content found")
        return None

    print(f"   Processing {len(df)} documents...")

    # Prepare documents and metadata
    documents = []
    metadatas = []

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
        content = str(row['content']).strip()

        # Skip empty or very short content
        if len(content) < 50:
            continue

        documents.append(content)

        metadata = {
            'title': str(row['title'])[:200],
            'author': str(row['author'])[:100],
            'source_type': str(row['source_type'])[:50],
            'url': str(row['url'])[:500],
            'word_count': int(row['word_count']),
            'import_date': str(row['import_date']),
            'database_source': db_name
        }
        metadatas.append(metadata)

    if len(documents) == 0:
        print(f"   ‚ùå No valid documents after filtering")
        return None

    print(f"   Encoding {len(documents)} documents...")

    # Create embeddings
    embeddings = model.encode(
        documents,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    ).astype('float32')

    # Create FAISS index
    dimension = embeddings.shape[1]
    print(f"   Creating FAISS index (dimension: {dimension})...")

    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
    index.add(embeddings)

    # Save index
    index_file = output_dir / "faiss_index.bin"
    faiss.write_index(index, str(index_file))

    # Save metadata
    metadata_file = output_dir / "faiss_metadata.pkl"
    with open(metadata_file, 'wb') as f:
        pickle.dump({
            'documents': documents,
            'metadatas': metadatas,
            'dimension': dimension
        }, f)

    # Save statistics
    stats = {
        'name': db_name,
        'document_count': len(documents),
        'embedding_dimension': dimension,
        'source_types': list(set([m['source_type'] for m in metadatas])),
        'total_words': sum([m['word_count'] for m in metadatas]),
        'creation_date': pd.Timestamp.now().isoformat()
    }

    stats_file = output_dir / "database_stats.json"
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"   ‚úÖ Created: {db_name}")
    print(f"      Documents: {len(documents):,}")
    print(f"      Index size: {os.path.getsize(index_file) / (1024 * 1024):.1f} MB")

    return output_dir


def find_sqlite_databases(search_dir="."):
    """Find all SQLite databases in directory"""
    sqlite_dbs = []

    search_path = Path(search_dir)

    # Look for .db files
    for db_file in search_path.rglob("*.db"):
        if db_file.is_file() and db_file.stat().st_size > 0:
            sqlite_dbs.append(db_file)

    # Also look in zip_databases folder if it exists
    zip_db_dir = search_path / "zip_databases"
    if zip_db_dir.exists():
        for db_file in zip_db_dir.glob("*.db"):
            if db_file.is_file():
                sqlite_dbs.append(db_file)

    return sqlite_dbs


def main():
    """Main function to create vector databases"""

    print("=" * 70)
    print("üîß VECTOR DATABASE CREATION TOOL")
    print("=" * 70)

    # Output directory for vector databases
    output_dir = Path("vector_databases")
    output_dir.mkdir(exist_ok=True)

    # Find SQLite databases
    print("\nüîç Looking for SQLite databases...")
    sqlite_dbs = find_sqlite_databases()

    if not sqlite_dbs:
        print("‚ùå No SQLite databases found!")
        print("\nLooking in:")
        print(f"  - Current directory: {Path('.').absolute()}")
        print(f"  - Zip databases folder: {Path('zip_databases').absolute()}")

        # Check for specific database files
        possible_dbs = [
            "knowledge_base_clean.db",
            "combined_knowledge.db",
            "knowledge_docs_facebook_posts.db",
            "knowledge_docs_foundation.db",
            "knowledge_docs_gavin_menzies.db"
        ]

        print("\nLooking for specific databases:")
        for db_name in possible_dbs:
            db_path = Path(db_name)
            if db_path.exists():
                sqlite_dbs.append(db_path)
                print(f"  ‚úÖ Found: {db_name}")

    if not sqlite_dbs:
        print("\nüí° TIPS:")
        print("1. Run your database splitting script first")
        print("2. Make sure you have .db files in the project directory")
        print("3. Or specify the path to your main database below")

        custom_path = input("\nEnter path to a SQLite database (or press Enter to skip): ").strip()
        if custom_path:
            custom_path = Path(custom_path)
            if custom_path.exists():
                sqlite_dbs.append(custom_path)

    if not sqlite_dbs:
        print("\n‚ùå Cannot proceed without databases.")
        return

    print(f"\nüìÅ Found {len(sqlite_dbs)} SQLite databases:")
    for i, db_path in enumerate(sqlite_dbs, 1):
        size_mb = db_path.stat().st_size / (1024 * 1024)
        print(f"{i}. {db_path.name} ({size_mb:.1f} MB)")

    # Ask which databases to process
    print("\nüìã Select databases to convert (comma-separated numbers, or 'all'):")
    print("   Example: 1,3,4  or  all")

    selection = input("Selection: ").strip().lower()

    if selection == 'all':
        selected_indices = list(range(len(sqlite_dbs)))
    else:
        try:
            selected_indices = [int(idx.strip()) - 1 for idx in selection.split(',')]
            selected_indices = [idx for idx in selected_indices if 0 <= idx < len(sqlite_dbs)]
        except:
            print("‚ùå Invalid selection")
            return

    # Process selected databases
    print(f"\nüöÄ Processing {len(selected_indices)} databases...")

    successful_dbs = []

    for idx in selected_indices:
        db_path = sqlite_dbs[idx]
        db_name = db_path.stem  # Remove .db extension

        try:
            output_path = create_vector_database_from_sqlite(
                str(db_path),
                str(output_dir),
                db_name
            )

            if output_path:
                successful_dbs.append((db_name, output_path))

        except Exception as e:
            print(f"‚ùå Failed to process {db_name}: {e}")

    # Summary
    print("\n" + "=" * 70)
    print("‚úÖ SUMMARY")
    print("=" * 70)

    if successful_dbs:
        print(f"\nüéâ Successfully created {len(successful_dbs)} vector databases:")
        for db_name, db_path in successful_dbs:
            print(f"  üìÅ {db_name} -> {db_path}")

        print("\nüìä Database locations:")
        print(f"  Vector databases: {output_dir.absolute()}")

        # Show folder structure
        print("\nüìÅ Folder structure created:")
        for item in output_dir.iterdir():
            if item.is_dir():
                files = list(item.glob("*"))
                print(f"  ‚îú‚îÄ‚îÄ {item.name}/")
                for file in files:
                    size_kb = file.stat().st_size / 1024
                    print(f"  ‚îÇ   ‚îú‚îÄ‚îÄ {file.name} ({size_kb:.1f} KB)")

        print("\nüéØ NEXT STEPS:")
        print("1. Run the Streamlit app again:")
        print("   streamlit run scripts/5_dual_output_app.py")
        print("\n2. The databases should now appear in the sidebar")

        # Create a test query file
        test_file = Path("test_queries.txt")
        with open(test_file, 'w') as f:
            f.write("Test Queries for 1421 Research AI:\n")
            f.write("=" * 40 + "\n\n")
            f.write("1. What evidence exists for Chinese exploration?\n")
            f.write("2. Compare different sources about Zheng He\n")
            f.write("3. Show timeline of Ming dynasty voyages\n")
            f.write("4. What do academic sources say about 1421?\n")
            f.write("5. Analyze the credibility of different sources\n")

        print(f"\nüìù Test queries saved to: {test_file}")

    else:
        print("\n‚ùå No vector databases were created.")
        print("\nüí° Troubleshooting:")
        print("1. Check if your SQLite databases have valid content")
        print("2. Make sure the 'content' column has text data")
        print("3. Try running with a smaller database first")


if __name__ == "__main__":
    main()
