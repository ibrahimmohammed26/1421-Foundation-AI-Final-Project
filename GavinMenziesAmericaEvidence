
"""
"Who Discovered America?" Evidence Scraper - SIMPLIFIED VERSION
- Scrapes all chapter evidence from Gavin Menzies' America book
- Readable folder names based on page titles
- Clean CSV columns (no links, timestamps, or unnecessary metadata)
"""
from playwright.sync_api import sync_playwright
import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import random
import hashlib
import csv
from datetime import datetime
import zipfile


class AmericaEvidenceScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        self.errors = []
        self.scrape_log = []
        self.url_mapping = {}  # Store URL to folder name mapping
        self.section_counter = {}
        self.page_counter = 1  # Global page counter

    def create_folder_name(self, url, section_name):
        """Create readable folder name based on page title"""
        # Get the title from URL path
        path = urlparse(url).path.strip('/').split('/')[-1]
        # Decode URL encoded characters
        path = path.replace('%e2%80%93', '-').replace('%e2%80%99', "'")

        # Convert to readable format
        title_words = path.replace('-', ' ').replace('chapter', 'Chapter').title()

        # Add section counter for uniqueness
        if section_name not in self.section_counter:
            self.section_counter[section_name] = 0
        self.section_counter[section_name] += 1

        # Extract section number (e.g., "01" from "01_prologue")
        section_num = section_name.split('_')[0]

        # Create folder name: SectionNumber_Title_Number
        folder_name = f"{section_num}_{title_words}_{self.section_counter[section_name]:02d}"

        # Truncate if too long
        if len(folder_name) > 60:
            folder_name = folder_name[:55]

        # Add short hash for uniqueness
        url_hash = hashlib.md5(url.encode()).hexdigest()[:4]
        folder_name = f"{folder_name}_{url_hash}"

        # Replace any remaining problematic characters
        folder_name = re.sub(r'[<>:"/\\|?*]', '_', folder_name)

        self.url_mapping[folder_name] = url
        return folder_name

    def scrape_all_evidence(self):
        """Main function to scrape all evidence pages"""
        print("ğŸš€ Starting 'Who Discovered America?' Evidence Scraping...")
        print("=" * 70)

        evidence_structure = self.get_evidence_structure()
        total_urls = sum(len(urls) for urls in evidence_structure.values())

        print(f"ğŸ“Š Total Evidence Pages: {total_urls}")
        print("=" * 70)

        successful = 0
        start_time = time.time()

        # Create main summary CSV data
        main_summary_data = []

        for section_name, urls in evidence_structure.items():
            print(f"\nğŸ“ SECTION: {section_name.upper()}")
            print("-" * 50)

            for idx, url in enumerate(urls, 1):
                print(f"  [{idx}/{len(urls)}] {url[:70]}...")
                folder_name = self.create_folder_name(url, section_name)
                time.sleep(random.uniform(1.5, 3))

                result = self.scrape_single_url(url, section_name, folder_name, idx)

                if result['success']:
                    successful += 1
                    print(f"  âœ… Success ({result.get('word_count', 0)} words)")

                    # Add to main summary data
                    main_summary_data.append({
                        'page_number': result.get('page_number', 0),
                        'folder_name': folder_name,
                        'url': url,
                        'title': result.get('title', ''),
                        'word_count': result.get('word_count', 0),
                        'section': section_name
                    })

                    self.scrape_log.append({
                        'section': section_name,
                        'url': url,
                        'status': 'success',
                        'words': result.get('word_count', 0)
                    })
                else:
                    print(f"  âŒ Failed: {result.get('error', 'Unknown')}")
                    self.errors.append({
                        'section': section_name,
                        'url': url,
                        'folder_name': folder_name,
                        'error': result.get('error', 'Unknown')
                    })

                    # Add failed entry to summary
                    main_summary_data.append({
                        'page_number': self.page_counter,
                        'folder_name': folder_name,
                        'url': url,
                        'title': '',
                        'word_count': 0,
                        'section': section_name
                    })

                    self.scrape_log.append({
                        'section': section_name,
                        'url': url,
                        'status': 'failed',
                        'error': result.get('error')
                    })

                self.page_counter += 1
                time.sleep(1)

        elapsed = time.time() - start_time

        print("\n" + "=" * 70)
        print(f"ğŸ‰ SCRAPING COMPLETED!")
        print(f"   Successful: {successful}/{total_urls}")
        print(f"   Failed: {len(self.errors)}/{total_urls}")
        print(f"   Time: {int(elapsed / 60)}m {int(elapsed % 60)}s")
        print("=" * 70)

        # Save all CSVs
        self.save_all_csvs(main_summary_data, successful, total_urls, elapsed)

        # Create zip file
        zip_path = self.create_zip_file()

        return successful, zip_path

    def save_all_csvs(self, summary_data, successful, total, elapsed):
        """Save all CSV files"""
        print("\nğŸ’¾ Saving all CSV files...")

        # 1. Main summary CSV
        self.save_summary_csv(summary_data)

        # 2. URL mapping CSV
        self.save_url_mapping_csv(summary_data)

        # 3. Scraping summary CSV
        self.save_scraping_summary_csv(successful, total, elapsed)

        # 4. Section summary CSV
        self.save_section_summary_csv(summary_data)

        # 5. Errors CSV (if any)
        if self.errors:
            self.save_errors_csv()

    def save_summary_csv(self, data):
        """Save main summary CSV"""
        csv_path = "america_evidence_scraped/all_evidence_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'page_number',
                    'folder_name',
                    'url',
                    'title',
                    'word_count',
                    'section'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            print(f"âœ… Main summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving main summary CSV: {e}")
            return False

    def save_url_mapping_csv(self, summary_data):
        """Save URL mapping CSV"""
        csv_path = "america_evidence_scraped/url_mapping.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['folder_name', 'url', 'title']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Create mapping from summary data
                for item in summary_data:
                    if item['title']:  # Only include successful scrapes
                        writer.writerow({
                            'folder_name': item['folder_name'],
                            'url': item['url'],
                            'title': item['title']
                        })

            print(f"âœ… URL mapping CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving URL mapping CSV: {e}")
            return False

    def save_scraping_summary_csv(self, successful, total, elapsed):
        """Save scraping summary CSV"""
        csv_path = "america_evidence_scraped/scraping_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'total_pages',
                    'successful',
                    'failed',
                    'success_rate',
                    'time_elapsed_minutes',
                    'date_completed'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow({
                    'total_pages': total,
                    'successful': successful,
                    'failed': len(self.errors),
                    'success_rate': f"{(successful / total * 100):.1f}%",
                    'time_elapsed_minutes': f"{int(elapsed / 60)}m {int(elapsed % 60)}s",
                    'date_completed': datetime.now().strftime("%Y-%m-%d")
                })

            print(f"âœ… Scraping summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving scraping summary CSV: {e}")
            return False

    def save_section_summary_csv(self, summary_data):
        """Save section summary CSV"""
        csv_path = "america_evidence_scraped/section_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            section_stats = {}

            # Calculate section statistics
            for item in summary_data:
                section = item['section']
                if section not in section_stats:
                    section_stats[section] = {'total': 0, 'success': 0, 'failed': 0}

                section_stats[section]['total'] += 1
                if item['word_count'] > 0:
                    section_stats[section]['success'] += 1
                else:
                    section_stats[section]['failed'] += 1

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'total_pages', 'successful', 'failed', 'success_rate']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for section, stats in section_stats.items():
                    writer.writerow({
                        'section': section,
                        'total_pages': stats['total'],
                        'successful': stats['success'],
                        'failed': stats['failed'],
                        'success_rate': f"{(stats['success'] / stats['total'] * 100):.1f}%" if stats['total'] > 0 else "0%"
                    })

            print(f"âœ… Section summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving section summary CSV: {e}")
            return False

    def save_errors_csv(self):
        """Save errors CSV"""
        csv_path = "america_evidence_scraped/scraping_errors.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'url', 'folder_name', 'error']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for err in self.errors:
                    writer.writerow({
                        'section': err['section'],
                        'url': err['url'],
                        'folder_name': err['folder_name'],
                        'error': err['error'][:500]
                    })

            print(f"âœ… Errors CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving errors CSV: {e}")
            return False

    def get_evidence_structure(self):
        """Evidence URLs - ALL URLs from the America book evidence page"""
        from collections import OrderedDict

        structure = OrderedDict()

        # Prologue
        structure["01_prologue"] = [
            "https://www.gavinmenzies.net/Evidence/prologue-%e2%80%93-life-at-sea/"
        ]

        # Chapter 1
        structure["02_chapter_01"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-1-%e2%80%93-a-land-bridge-too-far/"
        ]

        # Chapter 2
        structure["03_chapter_02"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-2-%e2%80%93-along-the-silk-road/"
        ]

        # Chapter 3
        structure["04_chapter_03"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-3-plants-between-continents/"
        ]

        # Chapter 4
        structure["05_chapter_04"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-4-%e2%80%93-european-seafaring-100-000-bc/"
        ]

        # Chapter 5
        structure["06_chapter_05"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-5-%e2%80%93-mastery-of-the-oceans-before-columbus/"
        ]

        # Chapter 6
        structure["07_chapter_06"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-6-the-genetic-evidence/"
        ]

        # Chapter 7
        structure["08_chapter_07"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-7-%e2%80%93-in-search-of-lost-civilisations-2/"
        ]

        # Chapter 8
        structure["09_chapter_08"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-8-%e2%80%93-the-olmec-the-foundation-culture-of-central-america/"
        ]

        # Chapter 9
        structure["10_chapter_09"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-9-%e2%80%93-pyramids-in-mexico-and-central-america/"
        ]

        # Chapter 10
        structure["11_chapter_10"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-10-%e2%80%93-pyramid-builders-of-south-america/"
        ]

        # Chapter 11
        structure["12_chapter_11"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-11-kubilai-khan%e2%80%99s-lost-fleets/"
        ]

        # Chapter 12
        structure["13_chapter_12"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-12-the-1418-chinese-map-of-the-world/"
        ]

        # Chapter 13
        structure["14_chapter_13"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-13-north-carolina-and-the-virginias/"
        ]

        # Chapter 14
        structure["15_chapter_14"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-14-the-eastern-seaboard/"
        ]

        # Chapter 15
        structure["16_chapter_15"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-15-nova-cataia-the-island-of-seven-cities/"
        ]

        # Chapter 16
        structure["17_chapter_16"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-16-the-pacific-coast-of-north-america/"
        ]

        # Chapter 17
        structure["18_chapter_17"] = [
            "https://www.gavinmenzies.net/Evidence/chapter-17-stone-age-sailors-%e2%80%93-the-%e2%80%9cwindover-bog%e2%80%9d-people-of-florida/"
        ]

        # Conclusion
        structure["19_conclusion"] = [
            "https://www.gavinmenzies.net/Evidence/conclusion-%e2%80%93-who-discovered-america/"
        ]

        return structure

    def scrape_single_url(self, url, section, folder_name, section_index):
        """Scrape a single URL and save as CSV"""
        try:
            base_dir = "america_evidence_scraped"
            # Create folder structure
            page_dir = os.path.join(base_dir, section, folder_name)
            data_dir = os.path.join(page_dir, "data")
            os.makedirs(data_dir, exist_ok=True)

            # Try scraping with requests first
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
            except:
                # Fallback to Playwright
                soup = self.scrape_playwright(url)

            title = self.clean_text(self.extract_title(soup, url))
            content = self.clean_text(self.extract_content(soup))
            author = self.extract_author(soup)

            # Get metadata
            word_count = len(content.split())

            # Create CSV data with simplified columns
            csv_data = {
                'page_number': self.page_counter,
                'folder_name': folder_name,
                'url': url,
                'title': title[:1000],
                'content': content[:50000],
                'author': author[:200],
                'word_count': word_count
            }

            # Save CSV with folder name as filename
            csv_path = os.path.join(data_dir, f"{folder_name}.csv")
            success = self.save_page_to_csv(csv_data, csv_path)

            if success:
                return {
                    'success': True,
                    'page_number': self.page_counter,
                    'word_count': word_count,
                    'title': title,
                    'content': content,
                    'author': author
                }
            else:
                return {'success': False, 'error': 'Failed to save CSV'}

        except Exception as e:
            print(f"    âš ï¸ Scraping error: {str(e)[:100]}")
            return {'success': False, 'error': str(e)}

    def scrape_playwright(self, url):
        """Fallback scraping with Playwright"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until="networkidle", timeout=60000)
            page.wait_for_timeout(3000)
            content = page.content()
            browser.close()
            return BeautifulSoup(content, 'html.parser')

    def extract_title(self, soup, url):
        """Extract page title"""
        t = soup.find('title')
        if t:
            title_text = t.get_text().strip()
            # Clean up title
            title_text = title_text.replace(' â€“ ', ' - ').replace(' | ', ' - ')
            return title_text

        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()

        # Try other heading tags
        for tag in ['h2', 'h3', 'h4']:
            heading = soup.find(tag)
            if heading:
                return heading.get_text().strip()

        # Use URL as fallback
        path = urlparse(url).path.strip('/').split('/')[-1]
        title = path.replace('-', ' ').replace('%e2%80%93', '-').title()
        return title.replace('Chapter ', 'Chapter ')

    def extract_author(self, soup):
        """Extract author from page"""
        # Try common author selectors
        author_selectors = [
            '.author',
            '.byline',
            '.post-author',
            '.entry-author',
            'meta[name="author"]',
            'meta[property="article:author"]'
        ]

        for sel in author_selectors:
            elem = soup.select_one(sel)
            if elem:
                if elem.name == 'meta':
                    return elem.get('content', '').strip()
                return elem.get_text().strip()

        return "Gavin Menzies"  # Default author for this site

    def extract_content(self, soup):
        """Extract main content from page"""
        # Remove unwanted elements
        for s in soup(["script", "style", "nav", "header", "footer", "iframe", "form", "button"]):
            s.decompose()

        # Try content selectors (priority order)
        content_selectors = [
            'main',
            'article',
            '.content',
            '#content',
            '.entry-content',
            'section',
            '.post-content',
            '.article-content',
            '.page-content',
            'div.content',
            'div.article'
        ]

        for sel in content_selectors:
            elem = soup.select_one(sel)
            if elem:
                text = elem.get_text(separator='\n', strip=True)
                if len(text) > 200:
                    return self.clean_content(text)

        # Fallback to body
        body = soup.find('body')
        if body:
            # Remove navigation and other noise
            for noise in body.select('nav, .navigation, .menu, .sidebar, .footer'):
                noise.decompose()
            text = body.get_text(separator='\n', strip=True)
            return self.clean_content(text)

        return "Content not found"

    def clean_content(self, text):
        """Clean extracted content"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def clean_text(self, text):
        """Clean text for safe CSV storage"""
        if not text:
            return ""

        # Replace problematic characters
        replacements = {
            '\u2022': 'â€¢',
            '\u2013': '-',
            '\u2014': '-',
            '\u2018': "'",
            '\u2019': "'",
            '\u201c': '"',
            '\u201d': '"',
            '\u00a0': ' ',
            '\u200b': '',  # Zero-width space
            '\ufffd': '',  # Replacement character
            '\u2026': '...',  # Ellipsis
            '\u00a9': '(c)',
            '\u00ae': '(R)',
            '\u2122': '(TM)',
            '"': '""',  # Escape double quotes for CSV
        }

        cleaned = text
        for char, replacement in replacements.items():
            cleaned = cleaned.replace(char, replacement)

        # Remove excessive whitespace but keep newlines for readability
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)

        return cleaned.strip()

    def save_page_to_csv(self, page_data, csv_path):
        """Save page data to CSV with simplified columns"""
        try:
            # Simplified column order as requested
            fieldnames = [
                'page_number',
                'folder_name',
                'url',
                'title',
                'content',
                'author',
                'word_count'
            ]

            # Write to CSV
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow(page_data)

            return True

        except Exception as e:
            print(f"    âš ï¸ Error saving CSV: {e}")
            return False

    def create_zip_file(self):
        """Create a zip file of all scraped evidence"""
        try:
            zip_path = "america_evidence_scraped.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk("america_evidence_scraped"):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, ".")
                        zipf.write(file_path, arcname)

            print(f"âœ… Zip file created: {zip_path}")
            return zip_path

        except Exception as e:
            print(f"âš ï¸ Could not create zip file: {e}")
            return None


def main():
    """Run the evidence scraper"""
    print("=" * 70)
    print("ğŸ“š WHO DISCOVERED AMERICA? EVIDENCE SCRAPER")
    print("   âœ“ Readable folder names based on page titles")
    print("   âœ“ Clean CSV columns (no timestamps, links, or unnecessary metadata)")
    print("   âœ“ Organized by chapter sections")
    print("=" * 70)

    print("\nğŸ“Š CSV COLUMNS FOR EVIDENCE PAGES:")
    print("   page_number - Sequential number in section")
    print("   folder_name - Readable folder name (e.g., 01_Prologue_Life_At_Sea_01_hash)")
    print("   url - Page URL")
    print("   title - Page title")
    print("   content - Main text content")
    print("   author - Author name")
    print("   word_count - Number of words")

    print("\nğŸ“ Output structure:")
    print("   america_evidence_scraped/")
    print("   â”œâ”€â”€ 01_prologue/")
    print("   â”‚   â””â”€â”€ 01_Prologue_Life_At_Sea_01_hash/")
    print("   â”‚       â””â”€â”€ data/01_Prologue_Life_At_Sea_01_hash.csv")
    print("   â”œâ”€â”€ 02_chapter_01/")
    print("   â”‚   â””â”€â”€ 02_Chapter_1_A_Land_Bridge_Too_Far_01_hash/")
    print("   â”‚       â””â”€â”€ data/02_Chapter_1_A_Land_Bridge_Too_Far_01_hash.csv")
    print("   â”œâ”€â”€ ... (17 chapters)")
    print("   â”œâ”€â”€ 19_conclusion/")
    print("   â”œâ”€â”€ all_evidence_summary.csv")
    print("   â”œâ”€â”€ url_mapping.csv")
    print("   â”œâ”€â”€ scraping_summary.csv")
    print("   â”œâ”€â”€ section_summary.csv")
    print("   â””â”€â”€ scraping_errors.csv (if any errors)")

    print(f"\nğŸ“Š Total sections: 19")
    print(f"ğŸ“Š Total evidence pages: 19 (Prologue + 17 Chapters + Conclusion)")

    print("\nğŸ“„ Chapters to scrape:")
    chapters = [
        "Prologue - Life at Sea",
        "Chapter 1 - A Land Bridge Too Far",
        "Chapter 2 - Along the Silk Road",
        "Chapter 3 - Plants Between Continents",
        "Chapter 4 - European Seafaring 100,000 BC",
        "Chapter 5 - Mastery of the Oceans Before Columbus",
        "Chapter 6 - The Genetic Evidence",
        "Chapter 7 - In Search of Lost Civilisations",
        "Chapter 8 - The Olmec",
        "Chapter 9 - Pyramids in Mexico and Central America",
        "Chapter 10 - Pyramid Builders of South America",
        "Chapter 11 - Kubilai Khan's Lost Fleets",
        "Chapter 12 - The 1418 Chinese Map of the World",
        "Chapter 13 - North Carolina and the Virginias",
        "Chapter 14 - The Eastern Seaboard",
        "Chapter 15 - Nova Cataia",
        "Chapter 16 - The Pacific Coast of North America",
        "Chapter 17 - Stone Age Sailors",
        "Conclusion - Who Discovered America?"
    ]

    for i, chapter in enumerate(chapters, 1):
        print(f"   {i:2d}. {chapter}")

    os.makedirs("america_evidence_scraped", exist_ok=True)

    # Ask for confirmation before starting
    print(f"\nâš ï¸  WARNING: This will scrape 19 evidence pages from gavinmenzies.net!")
    print("   Estimated time: 3-5 minutes")
    print("   Estimated storage: ~2-3MB")

    proceed = input("\nProceed with scraping? (y/n): ").strip().lower()
    if proceed != 'y':
        print("Scraping cancelled.")
        exit(0)

    print("\n" + "=" * 70)
    print("ğŸš€ Starting America evidence scraping...")
    print("â³ This will take a few minutes. Please be patient.")
    print("ğŸ“ All data will be saved as CSV files.")
    print("=" * 70)

    scraper = AmericaEvidenceScraper()
    success_count, zip_file = scraper.scrape_all_evidence()

    print("\nğŸ“Š FINAL SUMMARY")
    print(f"âœ… Successfully scraped: {success_count}/19 pages")
    print(f"ğŸ“‚ Output folder: america_evidence_scraped/")

    if zip_file:
        print(f"ğŸ“¦ Zip file: {zip_file}")

    print("\nğŸ“„ Output CSV files:")
    print("   - all_evidence_summary.csv (main index with page numbers, titles, word counts)")
    print("   - url_mapping.csv (folder name to URL mapping)")
    print("   - scraping_summary.csv (overall statistics)")
    print("   - section_summary.csv (per-section statistics)")

    if scraper.errors:
        print("   - scraping_errors.csv (error details)")
        print(f"\nâš ï¸  {len(scraper.errors)} pages failed to scrape")
        print("ğŸ’¡ Check scraping_errors.csv for details")

    print("\nâœ¨ All data saved in clean CSV format!")
    print("   Each page has its own folder with data.csv containing the content.")


if __name__ == "__main__":
    main()
