# 1_split_database.py
import sqlite3
import pandas as pd
import os
from pathlib import Path


def get_zip_file_from_source(source_zip):
    """Extract zip file name without extension"""
    if pd.isna(source_zip):
        return 'unknown'
    zip_name = str(source_zip)
    return Path(zip_name).stem  # Remove .zip extension


def split_database_by_zip_file():
    """Split the all_documents table into separate tables by zip file source"""

    print("=" * 70)
    print("ðŸ“Š SPLITTING DATABASE BY ZIP FILE SOURCE")
    print("=" * 70)

    # Connect to your database
    conn = sqlite3.connect('knowledge_base_clean.db')

    # Get unique zip files
    df = pd.read_sql_query("""
                           SELECT DISTINCT source_zip
                           FROM all_documents
                           WHERE source_zip IS NOT NULL
                             AND source_zip != ''
                           ORDER BY source_zip
                           """, conn)

    zip_files = df['source_zip'].tolist()

    print(f"Found {len(zip_files)} zip file sources:")
    for i, zip_file in enumerate(zip_files, 1):
        count = pd.read_sql_query(
            f"SELECT COUNT(*) as count FROM all_documents WHERE source_zip = ?",
            conn,
            params=(zip_file,)
        ).iloc[0]['count']
        print(f"  {i}. {zip_file}: {count:,} documents")

    # Create separate tables for each zip file
    for zip_file in zip_files:
        print(f"\nðŸ“ Creating table for: {zip_file}")

        # Get data for this zip file
        df_zip = pd.read_sql_query(
            f"SELECT * FROM all_documents WHERE source_zip = ?",
            conn,
            params=(zip_file,)
        )

        # Clean table name (remove special characters and .zip extension)
        table_name = f"docs_{get_zip_file_from_source(zip_file).lower()}"
        table_name = (table_name
        .replace(' ', '_')
        .replace('-', '_')
        .replace('.', '_')
        .replace('(', '')
        .replace(')', '')
        .replace('[', '')
        .replace(']', '')
        [:50])  # Limit length

        # Add zip file metadata
        df_zip['original_zip_file'] = zip_file
        df_zip['table_creation_date'] = pd.Timestamp.now()

        # Save to new table
        df_zip.to_sql(table_name, conn, if_exists='replace', index=False)

        # Add indexes
        cursor = conn.cursor()
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name}_title ON {table_name}(title)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name}_date ON {table_name}(import_date)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name}_type ON {table_name}(source_type)")

        # Create a summary row in metadata table
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS zip_file_metadata
                       (
                           id
                           INTEGER
                           PRIMARY
                           KEY
                           AUTOINCREMENT,
                           zip_file
                           TEXT
                           NOT
                           NULL,
                           table_name
                           TEXT
                           NOT
                           NULL,
                           document_count
                           INTEGER,
                           source_types
                           TEXT,
                           import_date
                           TIMESTAMP
                           DEFAULT
                           CURRENT_TIMESTAMP,
                           total_words
                           INTEGER,
                           avg_word_count
                           REAL
                       )
                       """)

        # Get summary stats
        source_types = ', '.join(df_zip['source_type'].unique().tolist())
        total_words = int(df_zip['word_count'].sum()) if 'word_count' in df_zip.columns else 0
        avg_word_count = float(df_zip['word_count'].mean()) if 'word_count' in df_zip.columns else 0

        cursor.execute("""
                       INSERT INTO zip_file_metadata
                       (zip_file, table_name, document_count, source_types, total_words, avg_word_count)
                       VALUES (?, ?, ?, ?, ?, ?)
                       """, (zip_file, table_name, len(df_zip), source_types, total_words, avg_word_count))

        print(f"  âœ… Created: {table_name} ({len(df_zip):,} rows)")

    # Create a summary view
    print("\nðŸ“ˆ Creating summary view...")

    summary_sql = """
                  CREATE VIEW IF NOT EXISTS vw_zip_file_summary AS
                  SELECT source_zip, \
                         COUNT(*)                           as document_count, \
                         GROUP_CONCAT(DISTINCT source_type) as source_types, \
                         SUM(word_count)                    as total_words, \
                         AVG(word_count)                    as avg_words, \
                         AVG(content_length)                as avg_length, \
                         MIN(import_date)                   as first_import, \
                         MAX(import_date)                   as last_import
                  FROM all_documents
                  WHERE source_zip IS NOT NULL
                  GROUP BY source_zip
                  ORDER BY document_count DESC \
                  """

    conn.execute(summary_sql)

    # Get final stats
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'docs_%'")
    doc_tables = [row[0] for row in cursor.fetchall()]

    print("\n" + "=" * 70)
    print("âœ… DATABASE SPLIT COMPLETE")
    print("=" * 70)

    print(f"\nðŸ“‹ Total document tables: {len(doc_tables)}")
    for table in sorted(doc_tables):
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        count = cursor.fetchone()[0]

        # Get zip file name from metadata
        cursor.execute(
            "SELECT zip_file FROM zip_file_metadata WHERE table_name = ?",
            (table,)
        )
        zip_info = cursor.fetchone()
        zip_name = zip_info[0] if zip_info else "Unknown"

        print(f"  ðŸ“Š {table}: {count:,} rows (from {Path(zip_name).name})")

    # Show summary
    print("\nðŸ“Š DOCUMENT SUMMARY BY ZIP FILE:")
    print("-" * 50)

    df_summary = pd.read_sql_query("SELECT * FROM vw_zip_file_summary", conn)
    for _, row in df_summary.iterrows():
        zip_name = Path(row['source_zip']).name
        print(f"{zip_name[:40]:40} {row['document_count']:>6,} docs")
        print(f"                    Types: {row['source_types'][:30]}...")
        print(f"                    Avg: {row['avg_words']:.0f} words")

    conn.close()

    return doc_tables


def create_individual_database_files():
    """Create separate .db files for each zip file table"""

    print("\n" + "=" * 70)
    print("ðŸ’¾ CREATING SEPARATE DATABASE FILES BY ZIP SOURCE")
    print("=" * 70)

    # Connect to main database
    main_conn = sqlite3.connect('knowledge_base_clean.db')

    # Get all document tables (starting with docs_)
    cursor = main_conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'docs_%'")
    doc_tables = [row[0] for row in cursor.fetchall()]

    # Create output directory for separate databases
    output_dir = Path("zip_databases")
    output_dir.mkdir(exist_ok=True)

    for table_name in doc_tables:
        # Get zip file info from metadata
        cursor.execute(
            "SELECT zip_file FROM zip_file_metadata WHERE table_name = ?",
            (table_name,)
        )
        zip_info = cursor.fetchone()
        zip_file_name = "unknown"
        if zip_info:
            zip_file_name = Path(zip_info[0]).stem

        # Create descriptive database filename
        db_filename = output_dir / f"knowledge_{zip_file_name}.db"

        # Read data from main database
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", main_conn)

        # Create new database
        new_conn = sqlite3.connect(db_filename)

        # Create documents table with all data
        df.to_sql('documents', new_conn, if_exists='replace', index=False)

        # Create additional metadata table
        metadata = pd.DataFrame([{
            'source_zip': zip_file_name,
            'original_table': table_name,
            'document_count': len(df),
            'export_date': pd.Timestamp.now(),
            'source_types': ', '.join(df['source_type'].unique().tolist()) if 'source_type' in df.columns else '',
            'total_words': df['word_count'].sum() if 'word_count' in df.columns else 0,
            'avg_word_count': df['word_count'].mean() if 'word_count' in df.columns else 0
        }])
        metadata.to_sql('database_metadata', new_conn, if_exists='replace', index=False)

        # Create source type breakdown table
        if 'source_type' in df.columns:
            type_breakdown = df['source_type'].value_counts().reset_index()
            type_breakdown.columns = ['source_type', 'count']
            type_breakdown.to_sql('source_type_breakdown', new_conn, if_exists='replace', index=False)

        # Create indexes
        cursor_new = new_conn.cursor()
        cursor_new.execute("CREATE INDEX IF NOT EXISTS idx_documents_title ON documents(title)")
        cursor_new.execute("CREATE INDEX IF NOT EXISTS idx_documents_import_date ON documents(import_date)")
        cursor_new.execute("CREATE INDEX IF NOT EXISTS idx_documents_source_type ON documents(source_type)")

        # Create views for easier querying
        cursor_new.execute("""
                           CREATE VIEW IF NOT EXISTS vw_document_stats AS
                           SELECT source_type,
                                  COUNT(*) as count,
                AVG(word_count) as avg_words,
                MIN(import_date) as oldest,
                MAX(import_date) as newest
                           FROM documents
                           GROUP BY source_type
                           ORDER BY count DESC
                           """)

        new_conn.commit()
        new_conn.close()

        print(f"âœ… Created: {db_filename.name} ({len(df):,} documents)")

    main_conn.close()

    print(f"\nðŸ“ Separate database files created in '{output_dir}/':")
    for db_file in output_dir.glob("*.db"):
        # Get file size
        size_mb = db_file.stat().st_size / (1024 * 1024)
        print(f"   {db_file.name} ({size_mb:.1f} MB)")


def create_combined_metadata_report():
    """Create a combined report of all zip file databases"""

    print("\n" + "=" * 70)
    print("ðŸ“‹ COMBINED METADATA REPORT")
    print("=" * 70)

    output_dir = Path("zip_databases")
    if not output_dir.exists():
        print("No separate databases found. Run create_individual_database_files() first.")
        return

    # Create a new database for the combined report
    combined_conn = sqlite3.connect(output_dir / "00_combined_metadata.db")

    all_metadata = []

    for db_file in output_dir.glob("knowledge_*.db"):
        if db_file.name == "00_combined_metadata.db":
            continue

        try:
            conn = sqlite3.connect(db_file)

            # Get database metadata
            df_meta = pd.read_sql_query("SELECT * FROM database_metadata", conn)
            if not df_meta.empty:
                metadata_row = df_meta.iloc[0].to_dict()
                metadata_row['database_file'] = db_file.name
                metadata_row['file_size_mb'] = db_file.stat().st_size / (1024 * 1024)

                # Get source type breakdown
                if 'source_type_breakdown' in [table[0] for table in
                                               pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'",
                                                                 conn).values]:
                    df_types = pd.read_sql_query("SELECT * FROM source_type_breakdown", conn)
                    metadata_row['source_type_details'] = df_types.to_json(orient='records')

                all_metadata.append(metadata_row)

            conn.close()
        except Exception as e:
            print(f"âš ï¸  Error reading {db_file.name}: {e}")

    if all_metadata:
        df_combined = pd.DataFrame(all_metadata)
        df_combined.to_sql('combined_metadata', combined_conn, if_exists='replace', index=False)

        # Create summary view
        cursor = combined_conn.cursor()
        cursor.execute("""
                       CREATE VIEW IF NOT EXISTS vw_database_summary AS
                       SELECT source_zip,
                              database_file,
                              document_count,
                              file_size_mb,
                              total_words,
                              avg_word_count,
                              export_date
                       FROM combined_metadata
                       ORDER BY document_count DESC
                       """)

        combined_conn.commit()

        print(f"âœ… Created combined metadata report: 00_combined_metadata.db")
        print(f"   Contains metadata for {len(all_metadata)} databases")

        # Display summary
        print("\nðŸ“Š DATABASE SUMMARY:")
        print("-" * 60)
        for _, row in df_combined.iterrows():
            print(f"{row['source_zip'][:30]:30} {row['document_count']:>6,} docs")
            print(f"  File: {row['database_file']}")

    combined_conn.close()


if __name__ == "__main__":
    # Step 1: Split into separate tables by zip file source
    tables = split_database_by_zip_file()

    # Step 2: Create separate database files
    create_separate = input("\nCreate separate .db files for each zip source? (y/n): ").strip().lower()
    if create_separate == 'y':
        create_individual_database_files()
        create_combined_metadata_report()

    print("\n" + "=" * 70)
    print("ðŸŽ¯ NEXT: CREATE VECTOR DATABASE")
    print("=" * 70)
    print("\nOptions:")
    print("1. Create vector database from combined knowledge_base_clean.db")
    print("2. Create vector database from individual zip file databases")
    print("3. Create separate vector databases for each zip file")
    print("\nRun: python 2_create_vector_db.py")
