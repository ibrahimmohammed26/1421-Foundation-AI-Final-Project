import sqlite3
import pickle
import re
import time
import pandas as pd
import numpy as np
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
import faiss


class FAISSVectorDatabase:
    def __init__(self, sqlite_db_path='knowledge_base_clean.db'):
        """Initialize FAISS vector database creator"""
        self.sqlite_db_path = sqlite_db_path
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.metadatas = []

    def initialize_model(self):
        """Load embedding model"""
        print("=" * 70)
        print("üß† INITIALIZING VECTOR DATABASE (FAISS)")
        print("=" * 70)

        print("\nüî§ Loading embedding model...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        dimension = self.embedding_model.get_sentence_embedding_dimension()
        print(f"‚úÖ Model loaded: {dimension} dimensions")

        # Initialize FAISS index
        self.index = faiss.IndexFlatL2(dimension)  # L2 distance
        print(f"‚úÖ FAISS index created")

        return dimension

    def clean_text(self, text: str, max_length: int = 2000) -> str:
        """Clean and prepare text for embedding"""
        if not text or not isinstance(text, str):
            return ""

        # Remove HTML tags
        text = re.sub(r'<[^>]+>', ' ', text)
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?\-:()\'"]', ' ', text)
        text = text.strip()

        # Truncate if too long
        if len(text) > max_length:
            text = text[:max_length] + "..."

        return text

    def prepare_document_for_embedding(self, document: Dict[str, Any]) -> str:
        """Prepare document text for embedding"""
        title = document.get('title', '')
        content = document.get('content', '')
        author = document.get('author', '')
        source_type = document.get('source_type', '')

        # Clean and combine
        cleaned_content = self.clean_text(content, max_length=1500)

        parts = []
        if title and len(title) > 5:
            parts.append(f"Title: {title}")
        if author and len(author) > 3:
            parts.append(f"Author: {author}")
        if source_type:
            parts.append(f"Source: {source_type}")
        parts.append(f"Content: {cleaned_content}")

        return "\n\n".join(parts)

    def create_embeddings_from_sqlite(self, batch_size: int = 50):
        """Create vector embeddings from SQLite database"""

        print("\nüì• Loading documents from SQLite...")

        # Connect to SQLite
        conn = sqlite3.connect(self.sqlite_db_path)

        # Get all documents
        df = pd.read_sql_query("""
                               SELECT id,
                                      source_type,
                                      title,
                                      content,
                                      url,
                                      author,
                                      word_count,
                                      page_number,
                                      folder_name,
                                      import_date
                               FROM all_documents
                               WHERE content IS NOT NULL
                                 AND LENGTH(content) > 100
                               ORDER BY id
                               """, conn)

        print(f"üìä Found {len(df):,} documents with sufficient content")

        # Process in batches
        total_processed = 0
        all_embeddings = []

        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i + batch_size]

            for _, row in batch.iterrows():
                doc_dict = row.to_dict()

                # Prepare document text
                doc_text = self.prepare_document_for_embedding(doc_dict)

                if len(doc_text) < 50:
                    continue

                # Generate embedding
                embedding = self.embedding_model.encode(doc_text)
                all_embeddings.append(embedding)

                # Store document and metadata
                self.documents.append(doc_text)

                metadata = {
                    "sqlite_id": int(doc_dict.get('id', 0)),
                    "source_type": doc_dict.get('source_type', 'unknown'),
                    "title": str(doc_dict.get('title', ''))[:200],
                    "author": str(doc_dict.get('author', ''))[:100],
                    "url": str(doc_dict.get('url', ''))[:500],
                    "word_count": int(doc_dict.get('word_count', 0)),
                    "folder_name": str(doc_dict.get('folder_name', ''))[:100],
                    "import_date": str(doc_dict.get('import_date', ''))
                }
                self.metadatas.append(metadata)

                total_processed += 1

            print(f"‚úÖ Processed {total_processed:,}/{len(df):,} documents")

        # Add all embeddings to FAISS index
        if all_embeddings:
            embeddings_array = np.array(all_embeddings).astype('float32')
            self.index.add(embeddings_array)
            print(f"\n‚úÖ Added {len(all_embeddings):,} vectors to FAISS index")

        conn.close()

        print("\n" + "=" * 70)
        print("üìä VECTOR DATABASE STATISTICS")
        print("=" * 70)
        print(f"üìÑ Total vectors: {self.index.ntotal:,}")
        print(f"‚úÖ Documents processed: {total_processed:,}")

        return total_processed

    def save_index(self, index_path="faiss_index.bin",
                   metadata_path="faiss_metadata.pkl"):
        """Save FAISS index and metadata to disk"""

        print(f"\nüíæ Saving FAISS index to {index_path}...")
        faiss.write_index(self.index, index_path)

        print(f"üíæ Saving metadata to {metadata_path}...")
        with open(metadata_path, 'wb') as f:
            pickle.dump({
                'documents': self.documents,
                'metadatas': self.metadatas
            }, f)

        print("‚úÖ Index and metadata saved successfully")

    def load_index(self, index_path="faiss_index.bin",
                   metadata_path="faiss_metadata.pkl"):
        """Load FAISS index and metadata from disk"""

        print(f"\nüìÇ Loading FAISS index from {index_path}...")
        self.index = faiss.read_index(index_path)

        print(f"üìÇ Loading metadata from {metadata_path}...")
        with open(metadata_path, 'rb') as f:
            data = pickle.load(f)
            self.documents = data['documents']
            self.metadatas = data['metadatas']

        print(f"‚úÖ Loaded {self.index.ntotal:,} vectors")

    def search(self, query: str, top_k: int = 5):
        """Search for similar documents"""

        # Generate query embedding
        query_embedding = self.embedding_model.encode(query).reshape(1, -1).astype('float32')

        # Search
        distances, indices = self.index.search(query_embedding, top_k)

        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                similarity = 1 / (1 + distances[0][i])  # Convert distance to similarity
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadatas[idx],
                    'similarity': similarity,
                    'distance': distances[0][i]
                })

        return results

    def test_search(self, test_queries: List[str] = None):
        """Test the vector search functionality"""

        if test_queries is None:
            test_queries = [
                "Zheng He Chinese explorer voyages",
                "Ming dynasty maritime exploration",
                "Gavin Menzies research evidence"
            ]

        print("\n" + "=" * 70)
        print("üîç TESTING VECTOR SEARCH")
        print("=" * 70)

        for query in test_queries:
            print(f"\nüîé Query: '{query}'")
            print("-" * 50)

            results = self.search(query, top_k=3)

            for i, result in enumerate(results):
                print(f"\nüìÑ Result {i + 1} (Similarity: {result['similarity']:.3f})")
                print(f"   Source: {result['metadata'].get('source_type', 'Unknown')}")
                print(f"   Title: {result['metadata'].get('title', 'No title')}")

                # Show preview
                preview = result['document'][:200] + "..." if len(result['document']) > 200 else result['document']
                lines = preview.split('\n')
                for line in lines[:3]:
                    if line.strip():
                        print(f"   {line[:80]}...")

    def export_info(self, output_file="vector_db_info.csv"):
        """Export information about stored vectors"""

        print(f"\nüíæ Exporting vector database info to {output_file}...")

        df = pd.DataFrame(self.metadatas)
        df.to_csv(output_file, index=False)
        print(f"‚úÖ Exported {len(df):,} vectors info")

        # Show summary
        print("\nüìä Vector Database Summary:")
        print("-" * 40)
        if 'source_type' in df.columns:
            source_counts = df['source_type'].value_counts()
            for source, count in source_counts.items():
                print(f"{source:25} {count:>6,}")

        return len(df)


def main():
    """Main function to create FAISS vector database"""

    # Initialize
    db = FAISSVectorDatabase('knowledge_base_clean.db')

    # Load model
    db.initialize_model()

    # Create embeddings
    print("\n" + "=" * 70)
    print("üß¨ CREATING VECTOR EMBEDDINGS")
    print("=" * 70)

    processed = db.create_embeddings_from_sqlite(batch_size=50)

    if processed > 0:
        # Save index
        db.save_index()

        # Test search
        db.test_search()

        # Export info
        db.export_info()

        print("\n" + "=" * 70)
        print("‚úÖ VECTOR DATABASE CREATION COMPLETE")
        print("=" * 70)
        print(f"\nüìÅ Files created:")
        print(f"   - faiss_index.bin (vector index)")
        print(f"   - faiss_metadata.pkl (document metadata)")
        print(f"   - vector_db_info.csv (summary)")
        print(f"\nüìä Total documents vectorized: {processed:,}")
        print(f"üî§ Embedding dimension: 384")
    else:
        print("‚ùå No documents were processed. Check your database.")


if __name__ == "__main__":
    main()
