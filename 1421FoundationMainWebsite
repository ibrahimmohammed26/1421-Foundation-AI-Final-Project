from playwright.sync_api import sync_playwright
import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import random
import hashlib
import csv
from datetime import datetime
import zipfile


class FoundationScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        self.errors = []
        self.scrape_log = []
        self.url_mapping = {}  # Store URL to readable name mapping
        self.section_counter = {}
        self.page_counter = 1  # Global page counter

    def create_folder_name(self, url, section_name):
        """Create readable folder name based on page title"""
        # Get the title from URL path
        path = urlparse(url).path.strip('/').split('/')[-1]
        if not path:
            title_words = "Homepage"
        else:
            # Convert to readable format: "about.html" -> "About"
            title_words = path.replace('.html', '').replace('.php', '').replace('-', ' ').replace('_', ' ').title()

        # Add section prefix and counter for uniqueness
        if section_name not in self.section_counter:
            self.section_counter[section_name] = 0
        self.section_counter[section_name] += 1

        # Format: SectionNumber_Title_Number
        section_num = section_name.split('_')[0]  # Get "01" from "01_main_pages"
        folder_name = f"{section_num}_{title_words}_{self.section_counter[section_name]:02d}"

        # Truncate if too long
        if len(folder_name) > 60:
            folder_name = folder_name[:55]

        # Add short hash for uniqueness
        url_hash = hashlib.md5(url.encode()).hexdigest()[:4]
        folder_name = f"{folder_name}_{url_hash}"

        # Replace any remaining problematic characters
        folder_name = re.sub(r'[<>:"/\\|?*]', '_', folder_name)

        self.url_mapping[folder_name] = url
        return folder_name

    def scrape_all_pages(self):
        """Main function to scrape all pages"""
        print("ğŸš€ Starting 1421 Foundation Website Scraping...")
        print("=" * 70)

        page_structure = self.get_page_structure()
        total_urls = sum(len(urls) for urls in page_structure.values())

        print(f"ğŸ“Š Total Pages: {total_urls}")
        print("=" * 70)

        successful = 0
        start_time = time.time()

        # Create main summary CSV data
        main_summary_data = []

        for section_name, urls in page_structure.items():
            print(f"\nğŸ“ SECTION: {section_name.upper()}")
            print("-" * 50)

            for idx, url in enumerate(urls, 1):
                print(f"  [{idx}/{len(urls)}] {url[:70]}...")
                folder_name = self.create_folder_name(url, section_name)
                time.sleep(random.uniform(1.5, 3))

                result = self.scrape_single_url(url, section_name, folder_name)

                if result['success']:
                    successful += 1
                    print(f"  âœ… Success ({result.get('word_count', 0)} words)")

                    # Add to main summary data
                    main_summary_data.append({
                        'page_number': self.page_counter,
                        'folder_name': folder_name,
                        'url': url,
                        'title': result.get('title', ''),
                        'word_count': result.get('word_count', 0),
                        'section': section_name
                    })

                    self.scrape_log.append({
                        'section': section_name,
                        'url': url,
                        'status': 'success',
                        'words': result.get('word_count', 0)
                    })

                    self.page_counter += 1
                else:
                    print(f"  âŒ Failed: {result.get('error', 'Unknown')}")
                    self.errors.append({
                        'section': section_name,
                        'url': url,
                        'folder_name': folder_name,
                        'error': result.get('error', 'Unknown')
                    })

                    # Add failed entry to summary
                    main_summary_data.append({
                        'page_number': self.page_counter,
                        'folder_name': folder_name,
                        'url': url,
                        'title': '',
                        'word_count': 0,
                        'section': section_name
                    })

                    self.scrape_log.append({
                        'section': section_name,
                        'url': url,
                        'status': 'failed',
                        'error': result.get('error')
                    })

                    self.page_counter += 1

                time.sleep(1)

        elapsed = time.time() - start_time

        print("\n" + "=" * 70)
        print(f"ğŸ‰ SCRAPING COMPLETED!")
        print(f"   Successful: {successful}/{total_urls}")
        print(f"   Failed: {len(self.errors)}/{total_urls}")
        print(f"   Time: {int(elapsed / 60)}m {int(elapsed % 60)}s")
        print("=" * 70)

        # Save all CSVs
        self.save_all_csvs(main_summary_data, successful, total_urls, elapsed)

        # Create zip file
        zip_path = self.create_zip_file()

        return successful, zip_path

    def save_all_csvs(self, summary_data, successful, total, elapsed):
        """Save all CSV files"""
        print("\nğŸ’¾ Saving all CSV files...")

        # 1. Main summary CSV
        self.save_summary_csv(summary_data)

        # 2. URL mapping CSV
        self.save_url_mapping_csv()

        # 3. Scraping summary CSV
        self.save_scraping_summary_csv(successful, total, elapsed)

        # 4. Section summary CSV
        self.save_section_summary_csv(summary_data)

        # 5. Errors CSV (if any)
        if self.errors:
            self.save_errors_csv()

    def save_summary_csv(self, data):
        """Save main summary CSV"""
        csv_path = "1421_foundation_scraped/all_pages_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'page_number',
                    'folder_name',
                    'url',
                    'title',
                    'word_count',
                    'section'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            print(f"âœ… Main summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving main summary CSV: {e}")
            return False

    def save_url_mapping_csv(self):
        """Save URL mapping CSV"""
        csv_path = "1421_foundation_scraped/url_mapping.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['folder_name', 'url', 'title']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Get titles from scraped data or extract from folder names
                for folder_name, url in self.url_mapping.items():
                    # Extract title from folder name
                    title_parts = folder_name.split('_')
                    # Remove section number and hash
                    title = ' '.join(title_parts[1:-2]).title()
                    writer.writerow({
                        'folder_name': folder_name,
                        'url': url,
                        'title': title
                    })

            print(f"âœ… URL mapping CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving URL mapping CSV: {e}")
            return False

    def save_scraping_summary_csv(self, successful, total, elapsed):
        """Save scraping summary CSV"""
        csv_path = "1421_foundation_scraped/scraping_summary.csv"
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'total_pages',
                    'successful',
                    'failed',
                    'success_rate',
                    'time_elapsed_minutes',
                    'date_completed'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow({
                    'total_pages': total,
                    'successful': successful,
                    'failed': len(self.errors),
                    'success_rate': f"{(successful / total * 100):.1f}%",
                    'time_elapsed_minutes': f"{int(elapsed / 60)}m {int(elapsed % 60)}s",
                    'date_completed': datetime.now().strftime("%Y-%m-%d")
                })

            print(f"âœ… Scraping summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving scraping summary CSV: {e}")
            return False

    def save_section_summary_csv(self, summary_data):
        """Save section summary CSV"""
        csv_path = "1421_foundation_scraped/section_summary.csv"
        try:
            section_stats = {}

            # Calculate section statistics
            for item in summary_data:
                section = item['section']
                if section not in section_stats:
                    section_stats[section] = {'total': 0, 'success': 0, 'failed': 0}

                section_stats[section]['total'] += 1
                if item['word_count'] > 0:
                    section_stats[section]['success'] += 1
                else:
                    section_stats[section]['failed'] += 1

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'total_pages', 'successful', 'failed', 'success_rate']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for section, stats in section_stats.items():
                    writer.writerow({
                        'section': section,
                        'total_pages': stats['total'],
                        'successful': stats['success'],
                        'failed': stats['failed'],
                        'success_rate': f"{(stats['success'] / stats['total'] * 100):.1f}%" if stats[
                                                                                                   'total'] > 0 else "0%"
                    })

            print(f"âœ… Section summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving section summary CSV: {e}")
            return False

    def save_errors_csv(self):
        """Save errors CSV"""
        csv_path = "1421_foundation_scraped/scraping_errors.csv"
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'url', 'folder_name', 'error']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for err in self.errors:
                    writer.writerow({
                        'section': err['section'],
                        'url': err['url'],
                        'folder_name': err['folder_name'],
                        'error': err['error'][:500]
                    })

            print(f"âœ… Errors CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving errors CSV: {e}")
            return False

    def get_page_structure(self):
        """1421 Foundation URLs - All Pages"""
        from collections import OrderedDict

        structure = OrderedDict()

        # Main navigation pages
        structure["01_main_pages"] = [
            "https://www.1421foundation.org/",
            "https://www.1421foundation.org/about.html",
            "https://www.1421foundation.org/research.html",
            "https://www.1421foundation.org/educate.html",
            "https://www.1421foundation.org/explore.html",
            "https://www.1421foundation.org/news.html",
            "https://www.1421foundation.org/contact.html",
            "https://www.1421foundation.org/donate.html"
        ]

        # Additional content pages found
        structure["02_content_pages"] = [
            "https://www.1421foundation.org/who_discovered_america.html",
            "https://www.1421foundation.org/christopher_columbus_legacy.html"
        ]

        # Research section (alternative URL)
        structure["03_research_section"] = [
            "https://www.1421foundation.org/research/"
        ]

        # News archives and blog
        structure["04_news_blog"] = [
            "https://www.1421foundation.org/news/read-our-latest-news",
            "https://www.1421foundation.org/news/archives/04-2025"
        ]

        return structure

    def scrape_single_url(self, url, section, folder_name):
        """Scrape a single URL and save as CSV"""
        try:
            base_dir = "1421_foundation_scraped"
            # Create folder structure
            page_dir = os.path.join(base_dir, section, folder_name)
            data_dir = os.path.join(page_dir, "data")
            os.makedirs(data_dir, exist_ok=True)

            # Try scraping with requests first
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
            except:
                # Fallback to Playwright
                soup = self.scrape_playwright(url)

            title = self.clean_text(self.extract_title(soup, url))
            content = self.clean_text(self.extract_content(soup))
            author = self.extract_author(soup)

            # Get metadata
            word_count = len(content.split())

            # Create CSV data with requested columns
            csv_data = {
                'page_number': self.section_counter[section],
                'folder_name': folder_name,
                'url': url,
                'title': title[:1000],
                'content': content[:50000],
                'author': author[:200],
                'word_count': word_count
            }

            # Save CSV
            csv_path = os.path.join(data_dir, f"{folder_name}.csv")
            success = self.save_page_to_csv(csv_data, csv_path)

            if success:
                return {
                    'success': True,
                    'word_count': word_count,
                    'title': title,
                    'content': content,
                    'author': author
                }
            else:
                return {'success': False, 'error': 'Failed to save CSV'}

        except Exception as e:
            print(f"    âš ï¸ Scraping error: {str(e)[:100]}")
            return {'success': False, 'error': str(e)}

    def scrape_playwright(self, url):
        """Fallback scraping with Playwright"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until="networkidle", timeout=60000)
            page.wait_for_timeout(3000)
            content = page.content()
            browser.close()
            return BeautifulSoup(content, 'html.parser')

    def extract_title(self, soup, url):
        """Extract page title"""
        t = soup.find('title')
        if t:
            return t.get_text().strip()

        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()

        # Try other heading tags
        for tag in ['h2', 'h3', 'h4']:
            heading = soup.find(tag)
            if heading:
                return heading.get_text().strip()

        # Use URL as fallback
        path = urlparse(url).path.strip('/').split('/')[-1]
        if not path:
            return "1421 Foundation Homepage"
        return path.replace('.html', '').replace('.php', '').replace('-', ' ').replace('_', ' ').title()

    def extract_author(self, soup):
        """Extract author from page"""
        # Try common author selectors
        author_selectors = [
            '.author',
            '.byline',
            '.post-author',
            '.entry-author',
            'meta[name="author"]',
            'meta[property="article:author"]',
            'meta[property="og:author"]'
        ]

        for sel in author_selectors:
            elem = soup.select_one(sel)
            if elem:
                if elem.name == 'meta':
                    return elem.get('content', '').strip()
                return elem.get_text().strip()

        return ""

    def extract_content(self, soup):
        """Extract main content from page"""
        # Remove unwanted elements
        for s in soup(["script", "style", "nav", "header", "footer", "iframe", "form", "button"]):
            s.decompose()

        # Try content selectors (priority order)
        content_selectors = [
            'main',
            'article',
            '.content',
            '#content',
            '.entry-content',
            'section',
            '.post-content',
            '.article-content',
            '.page-content',
            'div.content',
            'div.article',
            'body'
        ]

        for sel in content_selectors:
            elem = soup.select_one(sel)
            if elem:
                text = elem.get_text(separator='\n', strip=True)
                if len(text) > 100:
                    return self.clean_content(text)

        return "Content not found"

    def clean_content(self, text):
        """Clean extracted content"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def clean_text(self, text):
        """Clean text for safe CSV storage"""
        if not text:
            return ""

        # Replace problematic characters
        replacements = {
            '\u2022': 'â€¢',
            '\u2013': '-',
            '\u2014': '-',
            '\u2018': "'",
            '\u2019': "'",
            '\u201c': '"',
            '\u201d': '"',
            '\u00a0': ' ',
            '\u200b': '',  # Zero-width space
            '\ufffd': '',  # Replacement character
            '\u2026': '...',  # Ellipsis
            '\u00a9': '(c)',
            '\u00ae': '(R)',
            '\u2122': '(TM)',
            '\n': ' ',  # Replace newlines with spaces for CSV
            '\r': ' ',
            '\t': ' ',
            '"': '""',  # Escape double quotes for CSV
        }

        cleaned = text
        for char, replacement in replacements.items():
            cleaned = cleaned.replace(char, replacement)

        # Remove excessive whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned)

        return cleaned.strip()

    def save_page_to_csv(self, page_data, csv_path):
        """Save page data to CSV with requested columns"""
        try:
            # Requested column order
            fieldnames = [
                'page_number',
                'folder_name',
                'url',
                'title',
                'content',
                'author',
                'word_count'
            ]

            # Write to CSV
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow(page_data)

            return True

        except Exception as e:
            print(f"    âš ï¸ Error saving CSV: {e}")
            return False

    def create_zip_file(self):
        """Create a zip file of all scraped pages"""
        try:
            zip_path = "1421_foundation_scraped.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk("1421_foundation_scraped"):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, ".")
                        zipf.write(file_path, arcname)

            print(f"âœ… Zip file created: {zip_path}")
            return zip_path

        except Exception as e:
            print(f"âš ï¸ Could not create zip file: {e}")
            return None


def main():
    """Run the foundation scraper"""
    print("=" * 70)
    print("ğŸ“š 1421 FOUNDATION WEBSITE SCRAPER - SIMPLIFIED VERSION")
    print("   âœ“ Readable folder names based on page titles")
    print("   âœ“ Clean CSV columns")
    print("   âœ“ Organized by sections")
    print("=" * 70)

    print("\nğŸ“Š CSV COLUMNS FOR INDIVIDUAL PAGES:")
    print("   page_number - Sequential number in section")
    print("   folder_name - Readable folder name (e.g., 01_Homepage_01_abcd)")
    print("   url - Page URL")
    print("   title - Page title")
    print("   content - Main text content")
    print("   author - Author name (if available)")
    print("   word_count - Number of words")

    print("\nğŸ“ Output structure:")
    print("   1421_foundation_scraped/")
    print("   â”œâ”€â”€ 01_main_pages/")
    print("   â”‚   â”œâ”€â”€ 01_Homepage_01_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/01_Homepage_01_hash.csv")
    print("   â”‚   â”œâ”€â”€ 02_About_01_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/02_About_01_hash.csv")
    print("   â”‚   â””â”€â”€ ...")
    print("   â”œâ”€â”€ 02_content_pages/")
    print("   â”‚   â””â”€â”€ ...")
    print("   â”œâ”€â”€ 03_research_section/")
    print("   â”‚   â””â”€â”€ ...")
    print("   â”œâ”€â”€ 04_news_blog/")
    print("   â”‚   â””â”€â”€ ...")
    print("   â”œâ”€â”€ all_pages_summary.csv")
    print("   â”œâ”€â”€ url_mapping.csv")
    print("   â”œâ”€â”€ scraping_summary.csv")
    print("   â”œâ”€â”€ section_summary.csv")
    print("   â””â”€â”€ scraping_errors.csv (if any errors)")

    print(f"\nğŸ“Š Total sections: 4")
    print(f"ğŸ“Š Total pages: 13")
    print("\nğŸ“„ Pages to scrape:")
    print("\n   Section 1 - Main Pages (8):")
    print("   â€¢ Homepage")
    print("   â€¢ About")
    print("   â€¢ Research")
    print("   â€¢ Educate")
    print("   â€¢ Explore")
    print("   â€¢ News")
    print("   â€¢ Contact")
    print("   â€¢ Donate")
    print("\n   Section 2 - Content Pages (2):")
    print("   â€¢ Who Discovered America")
    print("   â€¢ Christopher Columbus Legacy")
    print("\n   Section 3 - Research Section (1):")
    print("   â€¢ Research (alternative page)")
    print("\n   Section 4 - News/Blog (2):")
    print("   â€¢ Read Our Latest News")
    print("   â€¢ News Archives (April 2025)")

    os.makedirs("1421_foundation_scraped", exist_ok=True)

    # Ask for confirmation before starting
    print(f"\nâš ï¸  This will scrape 13 pages from www.1421foundation.org")
    print("   Estimated time: 2-3 minutes")
    print("   Estimated storage: ~2-3MB")

    proceed = input("\nProceed with scraping? (y/n): ").strip().lower()
    if proceed != 'y':
        print("Scraping cancelled.")
        exit(0)

    print("\n" + "=" * 70)
    print("ğŸš€ Starting 1421 Foundation scraping...")
    print("ğŸ“ All data will be saved as CSV files.")
    print("=" * 70)

    scraper = FoundationScraper()
    success_count, zip_file = scraper.scrape_all_pages()

    print("\nğŸ“Š FINAL SUMMARY")
    print(f"âœ… Successfully scraped: {success_count}/13 pages")
    print(f"ğŸ“‚ Output folder: 1421_foundation_scraped/")

    if zip_file:
        print(f"ğŸ“¦ Zip file: {zip_file}")

    print("\nğŸ“„ Output CSV files:")
    print("   - all_pages_summary.csv (main index with page numbers, titles, word counts)")
    print("   - url_mapping.csv (folder name to URL mapping)")
    print("   - scraping_summary.csv (overall statistics)")
    print("   - section_summary.csv (per-section statistics)")

    if scraper.errors:
        print("   - scraping_errors.csv (error details)")
        print(f"\nâš ï¸  {len(scraper.errors)} pages failed to scrape")
        print("ğŸ’¡ Check scraping_errors.csv for details")

    print("\nâœ¨ All data saved in clean CSV format!")
    print("   Each page has its own folder with data.csv containing the content.")


if __name__ == "__main__":
    main()
