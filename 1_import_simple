import zipfile
import pandas as pd
import sqlite3
from pathlib import Path
import os

# ============================================
# STEP 1: ZIP FILE PATHS
# ============================================

ZIP_FILES = [
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\1421FacebookWebsite\facebook_pages_csv.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\1421FacebookWebsite\facebook_posts.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\1421FoundationWebsite\1421_foundation_scraped.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\GavinMenziesWebsite\1421_evidence_scraped.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\GavinMenziesWebsite\1434_evidence_scraped.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\GavinMenziesWebsite\america_evidence_scraped.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\GavinMenziesWebsite\atlantis_evidence_scraped.zip",
    r"C:\Users\ibrah\PycharmProjects\PythonProject12\raw_csvs\GavinMenziesWebsite\gavin_menzies_scraped.zip"
]


def create_database_tables(db_path='knowledge_base_clean.db'):
    """Create database with proper tables"""

    # Remove existing database
    if os.path.exists(db_path):
        try:
            os.remove(db_path)
            print("âœ… Removed old database")
        except:
            print("âš ï¸  Could not remove old database")

    # Create new database
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    print("ðŸ“‹ Creating database tables...")

    # Table 1: All Documents (combined view)
    cursor.execute("""
                   CREATE TABLE IF NOT EXISTS all_documents
                   (
                       id
                       INTEGER
                       PRIMARY
                       KEY
                       AUTOINCREMENT,
                       source_type
                       TEXT
                       NOT
                       NULL,
                       source_file
                       TEXT,
                       original_filename
                       TEXT,

                       -- Common fields
                       title
                       TEXT,
                       content
                       TEXT,
                       url
                       TEXT,

                       -- Additional fields
                       author
                       TEXT,
                       word_count
                       INTEGER,
                       page_number
                       INTEGER,
                       folder_name
                       TEXT,
                       import_date
                       TIMESTAMP
                       DEFAULT
                       CURRENT_TIMESTAMP,

                       -- Metadata
                       source_zip
                       TEXT,
                       has_content
                       BOOLEAN
                       DEFAULT
                       1,
                       content_length
                       INTEGER
                   )
                   """)

    # Table 2: Facebook-specific (detailed)
    cursor.execute("""
                   CREATE TABLE IF NOT EXISTS facebook_detailed
                   (
                       id
                       INTEGER
                       PRIMARY
                       KEY
                       AUTOINCREMENT,
                       document_id
                       INTEGER,
                       post_number
                       INTEGER,
                       post_date
                       TEXT,
                       post_time
                       TEXT,
                       image_count
                       INTEGER,
                       external_links_count
                       INTEGER,
                       FOREIGN
                       KEY
                   (
                       document_id
                   ) REFERENCES all_documents
                   (
                       id
                   )
                       )
                   """)

    # Table 3: File tracking
    cursor.execute("""
                   CREATE TABLE IF NOT EXISTS imported_files
                   (
                       id
                       INTEGER
                       PRIMARY
                       KEY
                       AUTOINCREMENT,
                       zip_file
                       TEXT
                       NOT
                       NULL,
                       csv_file
                       TEXT
                       NOT
                       NULL,
                       row_count
                       INTEGER,
                       import_date
                       TIMESTAMP
                       DEFAULT
                       CURRENT_TIMESTAMP,
                       UNIQUE
                   (
                       zip_file,
                       csv_file
                   )
                       )
                   """)

    # Create indexes
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_source_type ON all_documents(source_type)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_has_content ON all_documents(has_content)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_imported_files ON imported_files(zip_file, csv_file)")

    conn.commit()
    conn.close()

    print("âœ… Database tables created successfully")
    return db_path


def detect_source_type(filename, columns, zip_name):
    """
    Detect what type of file this is
    FIXED: Better detection logic with zip name context
    """
    filename_lower = filename.lower()
    zip_name_lower = zip_name.lower()
    columns_lower = [str(col).lower() for col in columns]

    # DEBUG: Print what we're detecting
    print(f"      Detecting: {filename}")
    print(f"      Columns: {columns_lower[:5]}...")  # Show first 5 columns
    print(f"      Zip: {zip_name}")

    # Priority 1: Facebook Posts (most specific first)
    if 'post_content' in columns_lower and 'post_url' in columns_lower:
        print(f"      â†’ Detected: facebook_posts")
        return 'facebook_posts'

    # Priority 2: Foundation website
    if 'section' in columns_lower and 'word_count' in columns_lower:
        print(f"      â†’ Detected: foundation")
        return 'foundation'

    # Priority 3: Facebook Links
    if 'link_url' in columns_lower and 'link_title' in columns_lower:
        print(f"      â†’ Detected: facebook_links")
        return 'facebook_links'

    # Priority 4: Facebook Pages (FIXED - check zip name)
    if 'facebook_pages' in zip_name_lower or 'facebook_page' in filename_lower:
        if 'content' in columns_lower or 'page_content' in columns_lower:
            print(f"      â†’ Detected: facebook_page")
            return 'facebook_page'

    # Priority 5: Gavin Menzies sites
    if any(keyword in zip_name_lower for keyword in ['evidence', 'menzies', 'atlantis', 'america']):
        if 'content' in columns_lower and 'url' in columns_lower:
            print(f"      â†’ Detected: gavin_menzies")
            return 'gavin_menzies'

    # Priority 6: Generic content with URL
    if 'content' in columns_lower and 'url' in columns_lower:
        print(f"      â†’ Detected: general")
        return 'general'

    # Priority 7: Title and content only
    if 'title' in columns_lower and 'content' in columns_lower:
        print(f"      â†’ Detected: general")
        return 'general'

    print(f"      â†’ Detected: UNKNOWN")
    return 'unknown'


def import_all_data(db_path='knowledge_base_clean.db'):
    """Import all data from ZIP files"""

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    print("\nðŸš€ IMPORTING ALL DATA")
    print("=" * 60)

    stats = {
        'total_rows': 0,
        'files_processed': 0,
        'files_skipped': 0,
        'by_source': {}
    }

    for zip_path in ZIP_FILES:
        zip_file = Path(zip_path)
        if not zip_file.exists():
            print(f"âŒ Missing: {zip_file.name}")
            continue

        print(f"\nðŸ“¦ Processing: {zip_file.name}")

        try:
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                csv_files = [f for f in zip_ref.namelist() if f.lower().endswith('.csv')]

                print(f"   Found {len(csv_files)} CSV files")

                for csv_file in csv_files:
                    try:
                        # Check if already imported
                        cursor.execute("""
                                       SELECT id
                                       FROM imported_files
                                       WHERE zip_file = ?
                                         AND csv_file = ?
                                       """, (zip_file.name, csv_file))

                        if cursor.fetchone():
                            print(f"   âš ï¸  Already imported: {Path(csv_file).name}")
                            stats['files_skipped'] += 1
                            continue

                        # Read CSV file
                        with zip_ref.open(csv_file) as f:
                            try:
                                df = pd.read_csv(f, encoding='utf-8')
                            except:
                                f.seek(0)
                                df = pd.read_csv(f, encoding='latin-1')

                        if len(df) == 0:
                            print(f"   âš ï¸  Empty: {Path(csv_file).name}")
                            stats['files_skipped'] += 1
                            continue

                        # FIXED: Pass zip_name to detect_source_type
                        source_type = detect_source_type(csv_file, df.columns, zip_file.name)

                        if source_type == 'unknown':
                            print(f"   âš ï¸  Unknown format: {Path(csv_file).name}")
                            print(f"      Columns found: {list(df.columns)[:5]}")
                            stats['files_skipped'] += 1
                            continue

                        # Import each row
                        rows_imported = 0
                        for idx, row in df.iterrows():
                            try:
                                # Extract common fields with better error handling
                                title = ""
                                content = ""
                                url = ""
                                author = ""
                                word_count = 0
                                page_number = 0
                                folder_name = ""

                                # FIXED: Better column mapping for Facebook Pages
                                if source_type == 'facebook_page':
                                    # Try multiple possible column names
                                    title = (str(row.get('page_title', '')) or
                                             str(row.get('title', '')) or
                                             str(row.get('Title', '')))

                                    content = (str(row.get('page_content', '')) or
                                               str(row.get('content', '')) or
                                               str(row.get('Content', '')) or
                                               str(row.get('text', '')))

                                    url = (str(row.get('page_url', '')) or
                                           str(row.get('url', '')) or
                                           str(row.get('URL', '')))

                                    author = (str(row.get('author', '')) or
                                              str(row.get('Author', '')) or
                                              '1421 Foundation')

                                    # Calculate word count if not provided
                                    if 'word_count' in df.columns and pd.notna(row['word_count']):
                                        word_count = int(row['word_count'])
                                    else:
                                        word_count = len(content.split()) if content else 0

                                    folder_name = str(row.get('folder_name', '')) or str(row.get('Folder', ''))

                                elif source_type == 'facebook_posts':
                                    title = str(row.get('post_title', '')) if pd.notna(row.get('post_title')) else ""
                                    content = str(row.get('post_content', '')) if pd.notna(
                                        row.get('post_content')) else ""
                                    url = str(row.get('post_url', '')) if pd.notna(row.get('post_url')) else ""
                                    author = str(row.get('post_author', '')) if pd.notna(row.get('post_author')) else ""
                                    word_count = int(row.get('word_count', 0)) if pd.notna(row.get('word_count')) else 0
                                    folder_name = str(row.get('folder_name', '')) if pd.notna(
                                        row.get('folder_name')) else ""

                                elif source_type == 'foundation':
                                    title = str(row.get('title', '')) if pd.notna(row.get('title')) else ""
                                    content = str(row.get('section', '')) if pd.notna(row.get('section')) else ""
                                    url = str(row.get('url', '')) if pd.notna(row.get('url')) else ""
                                    word_count = int(row.get('word_count', 0)) if pd.notna(row.get('word_count')) else 0
                                    page_number = int(row.get('page_number', 0)) if pd.notna(
                                        row.get('page_number')) else 0
                                    folder_name = str(row.get('folder_name', '')) if pd.notna(
                                        row.get('folder_name')) else ""

                                elif source_type == 'gavin_menzies':
                                    title = str(row.get('title', '')) if pd.notna(row.get('title')) else ""
                                    content = str(row.get('content', '')) if pd.notna(row.get('content')) else ""
                                    url = str(row.get('url', '')) if pd.notna(row.get('url')) else ""
                                    author = str(row.get('author', '')) if pd.notna(row.get('author')) else ""
                                    word_count = int(row.get('word_count', 0)) if pd.notna(row.get('word_count')) else 0
                                    page_number = int(row.get('page_number', 0)) if pd.notna(
                                        row.get('page_number')) else 0
                                    folder_name = str(row.get('folder_name', '')) if pd.notna(
                                        row.get('folder_name')) else ""

                                elif source_type == 'facebook_links':
                                    title = str(row.get('link_title', '')) if pd.notna(row.get('link_title')) else ""
                                    content = str(row.get('link_content', '')) if pd.notna(
                                        row.get('link_content')) else ""
                                    url = str(row.get('link_url', '')) if pd.notna(row.get('link_url')) else ""
                                    word_count = int(row.get('word_count', 0)) if pd.notna(row.get('word_count')) else 0

                                else:  # general
                                    title = str(row.get('title', '')) if pd.notna(row.get('title')) else ""
                                    content = str(row.get('content', '')) if pd.notna(row.get('content')) else ""
                                    url = str(row.get('url', '')) if pd.notna(row.get('url')) else ""
                                    author = str(row.get('author', '')) if pd.notna(row.get('author')) else ""
                                    word_count = int(row.get('word_count', 0)) if pd.notna(row.get('word_count')) else 0

                                # Clean up None values
                                title = title if title != 'nan' else ""
                                content = content if content != 'nan' else ""
                                url = url if url != 'nan' else ""
                                author = author if author != 'nan' else ""
                                folder_name = folder_name if folder_name != 'nan' else ""

                                # FIXED: Only skip if ALL fields are empty
                                # Allow import if ANY field has meaningful content
                                has_any_content = any([
                                    title and len(str(title).strip()) > 3,
                                    content and len(str(content).strip()) > 10,
                                    url and len(str(url).strip()) > 5,
                                    author and len(str(author).strip()) > 2,
                                    folder_name and len(str(folder_name).strip()) > 2
                                ])

                                if not has_any_content:
                                    continue

                                # Insert into main table
                                cursor.execute("""
                                               INSERT INTO all_documents
                                               (source_type, source_file, original_filename, title, content, url,
                                                author, word_count, page_number, folder_name, source_zip,
                                                content_length)
                                               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                               """, (
                                                   source_type,
                                                   csv_file,
                                                   Path(csv_file).name,
                                                   title[:500],
                                                   content,
                                                   url[:500],
                                                   author[:100],
                                                   word_count,
                                                   page_number,
                                                   folder_name[:100],
                                                   zip_file.name,
                                                   len(content)
                                               ))

                                rows_imported += 1
                                stats['total_rows'] += 1

                            except Exception as e:
                                # Show first error for debugging
                                if rows_imported == 0:
                                    print(f"      âš ï¸  Row error: {str(e)[:100]}")
                                continue

                        # Track imported file
                        if rows_imported > 0:
                            cursor.execute("""
                                           INSERT INTO imported_files (zip_file, csv_file, row_count)
                                           VALUES (?, ?, ?)
                                           """, (zip_file.name, csv_file, rows_imported))

                            # Update stats
                            if source_type not in stats['by_source']:
                                stats['by_source'][source_type] = 0
                            stats['by_source'][source_type] += rows_imported
                            stats['files_processed'] += 1

                            print(f"   âœ… {Path(csv_file).name}: {rows_imported:,} rows ({source_type})")
                        else:
                            print(f"   âš ï¸  {Path(csv_file).name}: 0 rows imported")
                            stats['files_skipped'] += 1

                    except Exception as e:
                        print(f"   âŒ Error in {Path(csv_file).name}: {str(e)[:100]}")
                        continue

        except Exception as e:
            print(f"âŒ Failed to process {zip_file.name}: {str(e)}")
            continue

    # Commit and close
    conn.commit()
    conn.close()

    return stats


def generate_verification_report(db_path='knowledge_base_clean.db'):
    """Generate a comprehensive verification report"""
    conn = sqlite3.connect(db_path)

    print("\n" + "=" * 70)
    print("ðŸ” COMPREHENSIVE VERIFICATION REPORT")
    print("=" * 70)

    cursor = conn.cursor()

    # Total documents
    cursor.execute("SELECT COUNT(*) FROM all_documents")
    total_docs = cursor.fetchone()[0]

    # Documents by source type
    cursor.execute("""
                   SELECT source_type,
                          COUNT(*) as count, 
               AVG(content_length) as avg_length,
               SUM(content_length) as total_chars
                   FROM all_documents
                   GROUP BY source_type
                   ORDER BY count DESC
                   """)

    print(f"\nðŸ“Š Total documents: {total_docs:,}\n")
    print("By source type:")
    print("-" * 40)
    for source_type, count, avg_len, total_chars in cursor.fetchall():
        print(f"  {source_type:20} {count:>8,} docs")
        print(f"                     Avg: {int(avg_len or 0):,} chars")

    conn.close()


if __name__ == "__main__":
    print("=" * 60)
    print("ðŸ“¦ FIXED CSV TO SQLITE CONVERTER - ACCEPTS PARTIAL DATA")
    print("=" * 60)

    # Step 1: Create database
    db_path = create_database_tables()

    # Step 2: Import all data
    stats = import_all_data(db_path)

    # Step 3: Show import summary
    print("\n" + "=" * 60)
    print("ðŸ“ˆ IMPORT SUMMARY")
    print("=" * 60)
    print(f"Total rows imported: {stats['total_rows']:,}")
    print(f"Files processed: {stats['files_processed']:,}")
    print(f"Files skipped: {stats['files_skipped']:,}")
    print("\nBy source type:")
    for source_type, count in stats['by_source'].items():
        print(f"  {source_type}: {count:,} rows")

    # Step 4: Verification
    generate_verification_report(db_path)

    print("\nâœ… Import complete! Database ready for vector conversion.")
