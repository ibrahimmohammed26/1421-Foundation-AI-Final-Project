"""
Enhanced 1421 Research AI with Multiple Database Support
- Automatic database detection with multiple fallbacks
- Debug mode to see what's happening
- Test database creation option

Run:
    streamlit run 5_dual_output_app.py
"""

import streamlit as st
import pickle
import faiss
from sentence_transformers import SentenceTransformer
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import re
from datetime import datetime
from pathlib import Path
import json
from typing import Dict, List, Tuple, Optional
import numpy as np
import sys
import os
import traceback


# Page config
st.set_page_config(
    page_title="1421 Research AI Assistant",
    page_icon="üß≠",
    layout="wide",
    initial_sidebar_state="expanded"
)


@st.cache_resource
def load_all_databases_with_fallbacks():
    """Load all available vector databases with multiple fallback locations"""

    databases = {}
    debug_info = []

    # Try multiple possible locations
    possible_locations = [
        Path("vector_databases"),
        Path("."),  # Current directory
        Path(".."),  # Parent directory
        Path("../vector_databases"),
        Path("scripts/vector_databases"),
        Path("C:/Users/ibrah/PycharmProjects/PythonProject12/vector_databases"),
        Path("C:/Users/ibrah/PycharmProjects/PythonProject12")
    ]

    debug_info.append("üîç Looking for vector databases in:")

    for location in possible_locations:
        location = location.resolve()
        debug_info.append(f"  - {location}")

        if location.exists():
            # Look for database folders
            for item in location.iterdir():
                db_info = None

                if item.is_dir():
                    # Check if it's a database folder
                    index_file = item / "faiss_index.bin"
                    metadata_file = item / "faiss_metadata.pkl"

                    if index_file.exists() and metadata_file.exists():
                        db_info = f"Found database folder: {item.name}"
                        try:
                            index = faiss.read_index(str(index_file))

                            with open(metadata_file, 'rb') as f:
                                data = pickle.load(f)
                                documents = data['documents']
                                metadatas = data['metadatas']

                            stats_file = item / "database_stats.json"
                            if stats_file.exists():
                                with open(stats_file, 'r') as f:
                                    stats = json.load(f)
                            else:
                                stats = {
                                    'name': item.name,
                                    'document_count': len(documents),
                                    'source_types': list(set([m.get('source_type', 'Unknown') for m in metadatas]))
                                }

                            databases[item.name] = {
                                'index': index,
                                'documents': documents,
                                'metadatas': metadatas,
                                'stats': stats,
                                'path': item
                            }

                            debug_info.append(f"    ‚úÖ Loaded: {item.name} ({len(documents)} docs)")

                        except Exception as e:
                            debug_info.append(f"    ‚ùå Error loading {item.name}: {str(e)}")

                elif item.name.endswith('.bin') and 'faiss' in item.name.lower():
                    # Single FAISS file
                    db_info = f"Found FAISS file: {item.name}"
                    try:
                        index = faiss.read_index(str(item))

                        # Look for metadata file
                        metadata_file_candidates = [
                            item.parent / item.name.replace('.bin', '.pkl'),
                            item.parent / 'faiss_metadata.pkl',
                            item.parent / 'metadata.pkl'
                        ]

                        metadata_file = None
                        for candidate in metadata_file_candidates:
                            if candidate.exists():
                                metadata_file = candidate
                                break

                        if metadata_file:
                            with open(metadata_file, 'rb') as f:
                                data = pickle.load(f)
                                documents = data['documents']
                                metadatas = data['metadatas']

                            db_name = item.stem
                            databases[db_name] = {
                                'index': index,
                                'documents': documents,
                                'metadatas': metadatas,
                                'stats': {
                                    'name': db_name,
                                    'document_count': len(documents),
                                    'source_types': list(set([m.get('source_type', 'Unknown') for m in metadatas]))
                                },
                                'path': item.parent
                            }

                            debug_info.append(f"    ‚úÖ Loaded single file DB: {db_name} ({len(documents)} docs)")
                        else:
                            debug_info.append(f"    ‚ö†Ô∏è  No metadata found for {item.name}")

                    except Exception as e:
                        debug_info.append(f"    ‚ùå Error loading FAISS file {item.name}: {str(e)}")

    # If no databases found, create a test database
    if not databases:
        debug_info.append("‚ö†Ô∏è No databases found. Creating test database...")
        test_db = create_test_database()
        if test_db:
            databases['test_database'] = test_db
            debug_info.append("‚úÖ Created test database")

    return databases, debug_info


def create_test_database():
    """Create a small test database for demonstration"""
    try:
        # Create test documents
        test_documents = [
            "Zheng He was a Chinese explorer who led voyages in the early 15th century.",
            "The 1421 hypothesis suggests Chinese discovered America before Columbus.",
            "Gavin Menzies wrote about Chinese exploration in his book 1421.",
            "There is debate among historians about Chinese discoveries.",
            "Chinese naval technology was advanced during the Ming dynasty."
        ]

        test_metadatas = [
            {'title': 'Zheng He Exploration', 'source_type': 'history', 'author': 'Historical Source'},
            {'title': '1421 Hypothesis', 'source_type': 'theory', 'author': 'Gavin Menzies'},
            {'title': 'Book Review', 'source_type': 'publication', 'author': 'Book Author'},
            {'title': 'Historical Debate', 'source_type': 'academic', 'author': 'Historian'},
            {'title': 'Naval Technology', 'source_type': 'technical', 'author': 'Researcher'}
        ]

        # Create embeddings
        model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = model.encode(test_documents).astype('float32')

        # Create FAISS index
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)

        return {
            'index': index,
            'documents': test_documents,
            'metadatas': test_metadatas,
            'stats': {
                'name': 'test_database',
                'document_count': len(test_documents),
                'source_types': ['history', 'theory', 'publication', 'academic', 'technical']
            },
            'path': Path('.')
        }

    except Exception as e:
        st.error(f"Failed to create test database: {e}")
        return None


@st.cache_resource
def load_sentence_model():
    """Load sentence transformer model"""
    try:
        return SentenceTransformer('all-MiniLM-L6-v2')
    except Exception as e:
        st.error(f"‚ùå Failed to load AI model: {e}")
        st.info("Please install: pip install sentence-transformers")
        return None


def simple_search(query, databases, model, top_k=5):
    """Simple search function for testing"""
    all_results = []

    for db_name, db_data in databases.items():
        index = db_data['index']
        documents = db_data['documents']
        metadatas = db_data['metadatas']

        try:
            # Encode query
            query_embedding = model.encode([query]).astype('float32')

            # Search
            k = min(top_k, len(documents))
            distances, indices = index.search(query_embedding, k)

            for idx, distance in zip(indices[0], distances[0]):
                if idx < len(documents):
                    similarity = 1 / (1 + distance)  # Simple similarity score
                    all_results.append({
                        'document': documents[idx],
                        'metadata': metadatas[idx],
                        'similarity': similarity,
                        'database': db_name
                    })

        except Exception as e:
            st.warning(f"Search error in {db_name}: {e}")

    # Sort by similarity
    all_results.sort(key=lambda x: x['similarity'], reverse=True)
    return all_results[:top_k]


def main():
    """Main Streamlit app"""

    # Custom CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        color: #4B5563;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton button {
        width: 100%;
    }
    .debug-box {
        background-color: #f0f0f0;
        padding: 10px;
        border-radius: 5px;
        border-left: 4px solid #666;
        font-family: monospace;
        font-size: 0.9em;
    }
    </style>
    """, unsafe_allow_html=True)

    # Header
    st.markdown('<h1 class="main-header">üß≠ 1421 Research AI Assistant</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Ask questions about 1421 hypothesis and Chinese exploration</p>', unsafe_allow_html=True)

    # Debug mode toggle
    debug_mode = st.sidebar.checkbox("üîß Debug Mode", value=True)

    # Load resources with progress indicator
    with st.spinner("Loading AI model and databases..."):
        databases, debug_info = load_all_databases_with_fallbacks()
        model = load_sentence_model()

    # Show debug info if enabled
    if debug_mode:
        with st.sidebar.expander("üîç Debug Information", expanded=True):
            st.write("Database loading details:")
            for line in debug_info:
                st.text(line)

            st.write(f"\nLoaded {len(databases)} database(s):")
            for db_name, db_data in databases.items():
                st.write(f"- {db_name}: {len(db_data['documents'])} documents")

    # Sidebar - Database Management
    with st.sidebar:
        st.header("üóÑÔ∏è Database Management")

        if not databases:
            st.error("""
            ‚ùå No databases found!
            
            To fix this:
            1. Run the database creation script first
            2. Or use the button below to create a test database
            3. Or manually create vector databases in the 'vector_databases' folder
            """)

            if st.button("üõ†Ô∏è Create Test Database", use_container_width=True):
                test_db = create_test_database()
                if test_db:
                    databases['test_database'] = test_db
                    st.success("‚úÖ Test database created!")
                    st.rerun()

            return

        # Database selector
        st.subheader("Select Database(s)")
        selected_dbs = st.multiselect(
            "Choose which databases to search:",
            options=list(databases.keys()),
            default=list(databases.keys()),
            help="Select one or more databases to search"
        )

        # Show database stats
        st.subheader("üìä Database Statistics")

        total_docs = sum([db['stats'].get('document_count', 0) for db in databases.values()])

        col1, col2 = st.columns(2)
        with col1:
            st.metric("Databases", len(databases))
        with col2:
            st.metric("Total Docs", total_docs)

        # Individual database info
        with st.expander("üìã Database Details"):
            for db_name in selected_dbs:
                if db_name in databases:
                    db_data = databases[db_name]
                    stats = db_data['stats']

                    st.markdown(f"**{db_name}**")
                    st.write(f"üìÑ Documents: {stats.get('document_count', 'N/A')}")
                    st.write(f"üìç Source types: {', '.join(stats.get('source_types', ['Unknown']))[:50]}...")
                    st.divider()

        # Quick actions
        st.divider()
        st.subheader("üöÄ Quick Actions")

        if st.button("üîÑ Refresh All", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()

    # Main content area
    if not selected_dbs:
        st.warning("Please select at least one database")
        return

    if not model:
        st.error("AI model failed to load. Please check installation.")
        return

    # Search interface
    st.header("üîç Ask Your Question")

    # Example queries
    examples = {
        "Basic": "Who was Zheng He?",
        "Theory": "What is the 1421 hypothesis?",
        "Evidence": "What evidence supports Chinese discovery of America?",
        "Timeline": "When did Chinese exploration occur?",
        "Comparison": "Compare Chinese and European exploration"
    }

    # Quick example buttons
    cols = st.columns(len(examples))
    for (example_name, example_text), col in zip(examples.items(), cols):
        with col:
            if st.button(example_name, use_container_width=True, key=f"btn_{example_name}"):
                st.session_state['current_query'] = example_text

    # Query input
    query = st.text_area(
        "Enter your question:",
        value=st.session_state.get('current_query', ''),
        height=100,
        placeholder="e.g., What evidence exists for Chinese exploration of America before Columbus?",
        key="query_input"
    )

    # Search button
    col1, col2 = st.columns([3, 1])
    with col2:
        search_clicked = st.button("üîé Search", type="primary", use_container_width=True)

    # Perform search
    if search_clicked and query:
        with st.spinner(f"Searching across {len(selected_dbs)} database(s)..."):
            # Filter selected databases
            selected_db_data = {k: v for k, v in databases.items() if k in selected_dbs}

            # Perform search
            results = simple_search(query, selected_db_data, model, top_k=10)

            if results:
                # Store in session state
                st.session_state['last_results'] = results
                st.session_state['last_query'] = query
                st.session_state['last_dbs'] = selected_dbs
            else:
                st.warning("No results found. Try a different query.")

    # Display results if available
    if 'last_results' in st.session_state:
        results = st.session_state['last_results']
        query = st.session_state['last_query']
        selected_dbs = st.session_state['last_dbs']

        # Results summary
        st.success(f"Found {len(results)} relevant document(s)")

        # Display results in a nice format
        for i, result in enumerate(results, 1):
            with st.expander(f"üìÑ {result['metadata'].get('title', 'Document ' + str(i))} (Relevance: {result['similarity']:.0%})", expanded=i==1):
                # Metadata
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Source", result['metadata'].get('source_type', 'Unknown'))
                with col2:
                    st.metric("Database", result['database'])
                with col3:
                    st.metric("Confidence", f"{result['similarity']:.0%}")

                # Content
                st.markdown("**Content:**")
                st.write(result['document'])

                # Show metadata details
                with st.expander("üìã Metadata Details"):
                    st.json(result['metadata'])

        # Analysis section
        st.divider()
        st.header("üìä Analysis")

        # Simple analysis
        if results:
            # Source type distribution
            source_types = [r['metadata'].get('source_type', 'Unknown') for r in results]
            source_counts = Counter(source_types)

            if len(source_counts) > 1:
                fig = px.pie(
                    values=list(source_counts.values()),
                    names=list(source_counts.keys()),
                    title="Source Type Distribution"
                )
                st.plotly_chart(fig, use_container_width=True)

            # Database distribution
            db_names = [r['database'] for r in results]
            db_counts = Counter(db_names)

            fig2 = px.bar(
                x=list(db_counts.keys()),
                y=list(db_counts.values()),
                title="Results by Database",
                labels={'x': 'Database', 'y': 'Count'}
            )
            st.plotly_chart(fig2, use_container_width=True)

    # Database creation instructions
    if not databases or len(databases) == 1 and 'test_database' in databases:
        st.divider()
        st.header("üõ†Ô∏è Setup Instructions")

        with st.expander("Click here for setup instructions"):
            st.markdown("""
            ### To create your own vector databases:
            
            1. **Run the database creation script:**
            ```bash
            python create_vector_dbs.py
            ```
            
            2. **Or create them manually:**
            - Create a folder called `vector_databases`
            - Inside, create subfolders for each database
            - Each subfolder needs:
              - `faiss_index.bin` (FAISS vector index)
              - `faiss_metadata.pkl` (document metadata)
            
            3. **Example structure:**
            ```
            vector_databases/
            ‚îú‚îÄ‚îÄ facebook_posts/
            ‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.bin
            ‚îÇ   ‚îî‚îÄ‚îÄ faiss_metadata.pkl
            ‚îú‚îÄ‚îÄ foundation_docs/
            ‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.bin
            ‚îÇ   ‚îî‚îÄ‚îÄ faiss_metadata.pkl
            ‚îî‚îÄ‚îÄ gavin_menzies/
                ‚îú‚îÄ‚îÄ faiss_index.bin
                ‚îî‚îÄ‚îÄ faiss_metadata.pkl
            ```
            
            4. **Restart this app** after creating databases
            """)

            if st.button("üìÅ Create Sample Folder Structure", use_container_width=True):
                # Create sample structure
                sample_dir = Path("vector_databases") / "sample_database"
                sample_dir.mkdir(parents=True, exist_ok=True)

                # Create sample files
                (sample_dir / "README.txt").write_text("""
                This is a sample database folder.
                
                To add your own database:
                1. Create faiss_index.bin using FAISS
                2. Create faiss_metadata.pkl with your documents
                3. Restart the Streamlit app
                """)

                st.success(f"Created sample folder at: {sample_dir}")
                st.info("Now add your FAISS index and metadata files to this folder")


def setup_instructions():
    """Display setup instructions"""
    st.markdown("""
    ## üöÄ Quick Start Guide
    
    1. **Install requirements:**
    ```bash
    pip install streamlit faiss-cpu sentence-transformers pandas plotly
    ```
    
    2. **Create vector databases:**
    ```bash
    python create_vector_dbs.py
    ```
    
    3. **Run the app:**
    ```bash
    streamlit run 5_dual_output_app.py
    ```
    
    4. **Access the app:** Open http://localhost:8501 in your browser
    """)


if __name__ == "__main__":
    # Check if running in Streamlit
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        if get_script_run_ctx():
            main()
        else:
            # Show instructions if running as script
            print("This is a Streamlit app.")
            print("Run with: streamlit run", __file__)
            setup_instructions()
    except:
        # Fallback
        main()
