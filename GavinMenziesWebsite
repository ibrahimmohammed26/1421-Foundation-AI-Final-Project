
from playwright.sync_api import sync_playwright
import os
import time
import re
from datetime import datetime
import zipfile
import csv
from urllib.parse import urlparse
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import json
from collections import OrderedDict


class GavinMenziesScraper:
    def __init__(self, max_workers=5):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        self.errors = []
        self.scrape_log = []
        self.url_mapping = {}
        self.max_workers = max_workers

        # Thread-safe locks
        self.errors_lock = threading.Lock()
        self.log_lock = threading.Lock()
        self.mapping_lock = threading.Lock()
        self.counter_lock = threading.Lock()

        # For maintaining order within each section
        self.section_order_data = {}  # Store data by section
        self.section_tasks = {}  # Store tasks by section
        self.current_global_page = 0  # Track global page number

        # Create main output directory
        self.base_dir = "gavin_menzies_scraped"
        os.makedirs(self.base_dir, exist_ok=True)

    def create_folder_name(self, url, section_name, position_in_section, page_title=""):
        """Create meaningful folder name with proper ordering"""
        # Extract meaningful name from URL path first
        path = urlparse(url).path.strip('/').split('/')[-1]
        if not path or path == 'index':
            # Try parent directory
            path_parts = urlparse(url).path.strip('/').split('/')
            if len(path_parts) > 1:
                path = path_parts[-2]

        # Try to use page title if available
        if page_title and len(page_title) > 5:
            # Clean the title
            clean_title = page_title.strip()
            # Remove website name and common suffixes
            clean_title = re.sub(r' - Gavin Menzies.*', '', clean_title, flags=re.IGNORECASE)
            clean_title = re.sub(r'\s*-\s*Gavin.*', '', clean_title, flags=re.IGNORECASE)
            clean_title = re.sub(r'\s*\|\s*.*', '', clean_title)

            # Create safe filename from title
            safe_name = re.sub(r'[^\w\s-]', '', clean_title)
            safe_name = re.sub(r'[-\s]+', '_', safe_name)
            safe_name = safe_name.strip('_').lower()

            if safe_name and len(safe_name) > 3:
                # Limit length but keep it meaningful
                if len(safe_name) > 40:
                    words = safe_name.split('_')
                    meaningful_words = []
                    for word in words:
                        if len(word) > 3 and word not in ['the', 'and', 'for', 'with', 'from', 'that']:
                            meaningful_words.append(word)
                        if len('_'.join(meaningful_words)) > 35:
                            break
                    safe_name = '_'.join(meaningful_words[:4])

                if safe_name and len(safe_name) > 3:
                    # Use position_in_section for ordering
                    section_num = section_name.split('_')[0]
                    folder_name = f"{section_num}_{position_in_section:03d}_{safe_name}"
                    return folder_name

        # If title doesn't work, use URL path
        if path:
            safe_name = re.sub(r'[^\w\s-]', '_', path)
            safe_name = re.sub(r'[-\s]+', '_', safe_name)
            safe_name = safe_name.strip('_').lower()

            # Clean up common patterns
            safe_name = re.sub(r'^page_|^p_|^index$', '', safe_name)

            if safe_name and len(safe_name) > 2:
                section_num = section_name.split('_')[0]
                folder_name = f"{section_num}_{position_in_section:03d}_{safe_name}"
                return folder_name

        # Fallback to section-based naming
        section_num = section_name.split('_')[0]
        # Use section theme for naming
        section_themes = {
            "01_main_website": "main",
            "02_america_book": "america",
            "03_atlantis_book": "atlantis",
            "04_china_main": "china",
            "05_speeches": "speech",
            "06_galleries": "gallery",
            "07_maps": "maps"
        }

        theme = section_themes.get(section_name, "page")
        return f"{section_num}_{position_in_section:03d}_{theme}"

    def get_website_structure(self):
        """Website URLs organized by section - ORDERED"""
        structure = OrderedDict()

        # Main Website Sections
        structure["01_main_website"] = [
            "https://www.gavinmenzies.net/",
            "https://www.gavinmenzies.net/blog/",
            "https://www.gavinmenzies.net/books/",
            "https://www.gavinmenzies.net/contact/",
            "https://www.gavinmenzies.net/contact/the-1421-research-education-and-exploration-foundation/",
            "https://www.gavinmenzies.net/contact/about/",
            "https://www.gavinmenzies.net/contact/about/awards-and-invitations-offered-for-1421-and-1434/",
            "https://www.gavinmenzies.net/contact/about/testimonials/",
            "https://www.gavinmenzies.net/contact/about/links/"
        ]

        # America Book Sections
        structure["02_america_book"] = [
            "https://www.gavinmenzies.net/america-book/about-who-discovered-america/",
            "https://www.gavinmenzies.net/america-book/extract/",
            "https://www.gavinmenzies.net/america-book/the-book/"
        ]

        # Lost Empire Atlantis Sections
        structure["03_atlantis_book"] = [
            "https://www.gavinmenzies.net/lost-empire-atlantis/about-atlantis/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/the-book/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/extract/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/gallery/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/gallery/gallery-1/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/gallery/gallery-2/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/gallery/gallery-3/",
            "https://www.gavinmenzies.net/lost-empire-atlantis/maps/"
        ]

        # China Sections (including 1434 and 1421 books)
        structure["04_china_main"] = [
            "https://www.gavinmenzies.net/china/about/",
            "https://www.gavinmenzies.net/china/1434-2/",
            "https://www.gavinmenzies.net/china/1434-2/extract/",
            "https://www.gavinmenzies.net/china/book-1421/",
            "https://www.gavinmenzies.net/china/book-1421/1421-extract/",
            "https://www.gavinmenzies.net/china/book-1421/discovery/",
            "https://www.gavinmenzies.net/china/book-1421/question-and-answers/",
            "https://www.gavinmenzies.net/china/book-1421/bibliography/",
            "https://www.gavinmenzies.net/china/gavin-menzies-videos/"
        ]

        # Gavin Menzies Speeches
        structure["05_speeches"] = [
            "https://www.gavinmenzies.net/china/book-1421/gavin-menzies-speeches/",
            "https://www.gavinmenzies.net/china/book-1421/gavin-menzies-speeches/talk-a-how-the-story-came-about/",
            "https://www.gavinmenzies.net/china/book-1421/gavin-menzies-speeches/talk-b-zheng-hes-fleets-voyages-from-south-america-to-new-zealand-and-australia/",
            "https://www.gavinmenzies.net/china/book-1421/gavin-menzies-speeches/talk-c-the-first-panama-canal-and-the-first-suez-canal-were-built-by-the-chinese-and-the-egyptians/",
            "https://www.gavinmenzies.net/china/book-1421/gavin-menzies-speeches/gavins-talk-at-the-ancestry-e-symposium/"
        ]

        # Gallery Sections
        structure["06_galleries"] = [
            "https://www.gavinmenzies.net/china/gallery/",
            "https://www.gavinmenzies.net/china/gallery/shipwreck/",
            "https://www.gavinmenzies.net/china/gallery/ceramics/",
            "https://www.gavinmenzies.net/china/gallery/jade/",
            "https://www.gavinmenzies.net/china/gallery/metal-artefacts/",
            "https://www.gavinmenzies.net/china/gallery/cave-art/",
            "https://www.gavinmenzies.net/china/gallery/stone-buildings-mortar-and-carvings/",
            "https://www.gavinmenzies.net/china/gallery/flora-and-fauna/",
            "https://www.gavinmenzies.net/china/gallery/miscellaneous-artefacts/",
            "https://www.gavinmenzies.net/china/gallery/mike-boss-artist-gallery/"
        ]

        # Maps Sections
        structure["07_maps"] = [
            "https://www.gavinmenzies.net/china/maps/",
            "https://www.gavinmenzies.net/china/maps/australia/",
            "https://www.gavinmenzies.net/china/maps/british-columbia/",
            "https://www.gavinmenzies.net/china/maps/caribbean/",
            "https://www.gavinmenzies.net/china/maps/pacific/",
            "https://www.gavinmenzies.net/china/maps/north-east-united-states/"
        ]

        return structure

    def scrape_all_pages(self):
        """Main function to scrape all website pages with threading but maintain order"""
        print("ğŸš€ Starting Gavin Menzies Website Scraping - THREADED WITH ORDER PRESERVATION")
        print("=" * 70)

        website_structure = self.get_website_structure()
        total_urls = sum(len(urls) for urls in website_structure.values())

        print(f"ğŸ“Š Total Website Pages: {total_urls}")
        print(f"ğŸ§µ Using {self.max_workers} threads for faster scraping")
        print("ğŸ“š Order will be preserved within each section")
        print("=" * 70)

        successful = 0
        start_time = time.time()

        # Initialize section order data storage
        for section_name in website_structure.keys():
            self.section_order_data[section_name] = []
            self.section_tasks[section_name] = []

        # Process sections sequentially, but URLs within each section in parallel
        completed = 0
        failed = 0

        print(f"\nâ³ Starting parallel scraping with {self.max_workers} workers...")
        print("ğŸ“Š Progress will be shown below:")

        # Process each section one at a time to maintain section order
        for section_idx, (section_name, urls) in enumerate(website_structure.items(), 1):
            print(f"\nğŸ“ SECTION {section_idx}/7: {section_name.upper()}")
            print("-" * 50)

            # Pre-create section directory
            section_dir = os.path.join(self.base_dir, section_name)
            os.makedirs(section_dir, exist_ok=True)

            # Create tasks for this section
            tasks = []
            for idx, url in enumerate(urls, 1):
                # Create temporary folder name with position
                section_num = section_name.split('_')[0]
                temp_folder_name = f"{section_num}_{idx:03d}_temp"

                task = {
                    'global_index': completed + idx,
                    'section_name': section_name,
                    'url': url,
                    'temp_folder_name': temp_folder_name,
                    'section_index': idx,
                    'position_in_section': idx,
                    'total_in_section': len(urls),
                    'section_num': section_num
                }
                tasks.append(task)

            # Store tasks for this section
            self.section_tasks[section_name] = tasks

            # Process this section's URLs in parallel
            section_successful = 0
            section_failed = 0

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {executor.submit(self.scrape_single_url_threaded, task): task for task in tasks}

                # Collect results as they complete
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        result = future.result(timeout=60)

                        with self.counter_lock:
                            if result['success']:
                                section_successful += 1
                                successful += 1
                                status = "âœ…"
                            else:
                                section_failed += 1
                                failed += 1
                                status = "âŒ"
                            completed += 1

                        # Store result in order based on position_in_section
                        while len(self.section_order_data[section_name]) < task['position_in_section']:
                            self.section_order_data[section_name].append(None)

                        # Store at correct position (0-based index)
                        self.section_order_data[section_name][task['position_in_section'] - 1] = {
                            'task': task,
                            'result': result
                        }

                        # Print progress
                        progress_percent = (completed / total_urls) * 100
                        if result['success']:
                            print(
                                f"  [{completed}/{total_urls}] {progress_percent:.1f}% - {status} {result['final_folder_name']} ({result['word_count']} words)")
                        else:
                            print(
                                f"  [{completed}/{total_urls}] {progress_percent:.1f}% - {status} Failed: {result.get('error', 'Unknown')[:50]}")

                    except Exception as e:
                        with self.counter_lock:
                            failed += 1
                            completed += 1
                        print(f"  âš ï¸ Task failed with exception: {str(e)[:50]}")

                        # Store failed result
                        while len(self.section_order_data[section_name]) < task['position_in_section']:
                            self.section_order_data[section_name].append(None)

                        self.section_order_data[section_name][task['position_in_section'] - 1] = {
                            'task': task,
                            'result': {
                                'success': False,
                                'error': str(e),
                                'final_folder_name': task['temp_folder_name']
                            }
                        }

            print(f"  ğŸ“Š Section complete: {section_successful}/{len(urls)} successful")

            # Small delay between sections
            if section_idx < len(website_structure):
                time.sleep(1)

        elapsed = time.time() - start_time

        print("\n" + "=" * 70)
        print(f"ğŸ‰ SCRAPING COMPLETED!")
        print(f"   Successful: {successful}/{total_urls}")
        print(f"   Failed: {failed}/{total_urls}")
        print(f"   Time: {int(elapsed / 60)}m {int(elapsed % 60)}s")
        print(f"   Speed: {total_urls / elapsed:.2f} pages/second")
        print("=" * 70)

        # Compile summary data in correct order
        all_summary_data = self.compile_ordered_summary_data()

        # Save all CSVs
        self.save_all_csvs(all_summary_data, successful, total_urls, elapsed)

        # Create zip file
        zip_path = self.create_zip_file()

        return successful, zip_path

    def compile_ordered_summary_data(self):
        """Compile summary data in correct order from stored results"""
        summary_data = []

        # Sort sections by their numeric prefix
        sorted_sections = sorted(self.section_order_data.keys(),
                                 key=lambda x: int(x.split('_')[0]))

        for section_name in sorted_sections:
            section_results = self.section_order_data[section_name]

            # Filter out None values
            valid_results = [r for r in section_results if r is not None]

            # Sort by position_in_section to ensure correct order
            valid_results.sort(key=lambda x: x['task']['position_in_section'])

            for item in valid_results:
                task = item['task']
                result = item['result']

                if result['success']:
                    summary_data.append({
                        'page_number': task['position_in_section'],
                        'folder_name': result['final_folder_name'],
                        'url': task['url'],
                        'title': result.get('title', ''),
                        'content': result.get('content', ''),
                        'author': result.get('author', ''),
                        'word_count': result.get('word_count', 0),
                        'section': section_name
                    })
                else:
                    summary_data.append({
                        'page_number': task['position_in_section'],
                        'folder_name': result.get('final_folder_name', task['temp_folder_name']),
                        'url': task['url'],
                        'title': '',
                        'content': '',
                        'author': '',
                        'word_count': 0,
                        'section': section_name
                    })

        return summary_data

    def scrape_single_url_threaded(self, task):
        """Threaded version of scrape_single_url"""
        url = task['url']
        section_name = task['section_name']
        position = task['position_in_section']
        temp_folder_name = task['temp_folder_name']

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = context.new_page()

                try:
                    # Navigate to page
                    page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    page.wait_for_timeout(1500)

                    # Scroll to load content
                    page.evaluate("window.scrollBy(0, 500)")
                    page.wait_for_timeout(500)

                    # Extract page data
                    page_data = self.extract_page_data(page, url)

                    # Extract author
                    author = self.extract_author(page)

                    # Create final folder name using actual title
                    final_folder_name = self.create_folder_name(
                        url,
                        section_name,
                        position,
                        page_data.get('title', '')
                    )

                    # Add hash for uniqueness
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:4]
                    final_folder_name = f"{final_folder_name}_{url_hash}"

                    # Create folder structure with final name
                    page_dir = os.path.join(self.base_dir, section_name, final_folder_name)
                    data_dir = os.path.join(page_dir, "data")
                    os.makedirs(data_dir, exist_ok=True)

                    # Save page content to CSV
                    csv_path = os.path.join(data_dir, f"{final_folder_name}.csv")
                    self.save_page_to_csv(
                        page_data,
                        csv_path,
                        final_folder_name,
                        position,
                        section_name,
                        author
                    )

                    # Store URL mapping
                    with self.mapping_lock:
                        self.url_mapping[final_folder_name] = url

                    browser.close()

                    return {
                        'success': True,
                        'final_folder_name': final_folder_name,
                        'url': url,
                        'title': page_data.get('title', ''),
                        'content': page_data.get('content', ''),
                        'author': author,
                        'word_count': page_data.get('word_count', 0),
                        'position': position
                    }

                except Exception as e:
                    browser.close()
                    raise Exception(f"Scraping error: {str(e)[:100]}")

        except Exception as e:
            error_msg = str(e)[:200]

            with self.errors_lock:
                self.errors.append({
                    'section': section_name,
                    'url': url,
                    'position': position,
                    'error': error_msg
                })

            return {
                'success': False,
                'error': error_msg,
                'position': position,
                'final_folder_name': temp_folder_name
            }

    def extract_page_data(self, page, url):
        """Extract data from a webpage"""
        data = {
            'url': url,
            'title': '',
            'content': '',
            'word_count': 0
        }

        try:
            # Get page title
            title = page.title()
            data['title'] = title.strip() if title else ""

            # Try to extract main content
            content_selectors = [
                'main',
                'article',
                'div.content',
                'div.entry-content',
                'div.post-content',
                'div.article-content',
                'div.page-content'
            ]

            content_text = ""
            for selector in content_selectors:
                try:
                    element = page.query_selector(selector)
                    if element:
                        text = element.inner_text().strip()
                        if len(text) > 100:
                            content_text = text
                            break
                except:
                    continue

            # If no content found with selectors, try paragraphs
            if not content_text or len(content_text) < 100:
                try:
                    paragraphs = page.query_selector_all('p')
                    texts = []
                    for p in paragraphs[:40]:
                        text = p.inner_text().strip()
                        if len(text) > 30:
                            texts.append(text)
                    content_text = '\n\n'.join(texts)
                except:
                    pass

            # If still no content, try body as last resort
            if not content_text or len(content_text) < 50:
                try:
                    body = page.query_selector('body')
                    if body:
                        body_text = body.inner_text()
                        # Clean up the text
                        content_text = re.sub(r'\n\s*\n', '\n\n', body_text)
                        content_text = re.sub(r'[ \t]+', ' ', content_text)
                except:
                    pass

            # Clean and store the content
            data['content'] = self.clean_text(content_text)
            data['word_count'] = len(data['content'].split())

        except Exception as e:
            print(f"      âš ï¸ Error in extraction: {e}")

        return data

    def extract_author(self, page):
        """Extract author from webpage"""
        try:
            # Try to find author using various selectors
            author_selectors = [
                'meta[name="author"]',
                'meta[property="article:author"]',
                'meta[property="og:author"]',
                '.author',
                '.byline',
                '.post-author',
                '.entry-author'
            ]

            for selector in author_selectors:
                try:
                    if selector.startswith('meta'):
                        element = page.query_selector(selector)
                        if element:
                            content = element.get_attribute('content')
                            if content and content.strip():
                                return content.strip()
                    else:
                        element = page.query_selector(selector)
                        if element:
                            text = element.inner_text().strip()
                            if text:
                                return text
                except:
                    continue

            # Check if Gavin Menzies is mentioned in the content
            content = page.query_selector('body')
            if content:
                body_text = content.inner_text().lower()
                if 'gavin menzies' in body_text:
                    return 'Gavin Menzies'

            return "Unknown"

        except Exception as e:
            return "Unknown"

    def clean_text(self, text):
        """Clean text for safe CSV storage"""
        if not text:
            return ""

        # Replace problematic characters
        replacements = {
            '\u2022': 'â€¢',
            '\u2013': '-',
            '\u2014': '-',
            '\u2018': "'",
            '\u2019': "'",
            '\u201c': '"',
            '\u201d': '"',
            '\u00a0': ' ',
            '\u200b': '',  # Zero-width space
            '\ufffd': '',  # Replacement character
            '\u2026': '...',  # Ellipsis
            '\u00a9': '(c)',
            '\u00ae': '(R)',
            '\u2122': '(TM)',
            '\n': ' ',  # Replace newlines with spaces for CSV
            '\r': ' ',
            '\t': ' ',
            '"': '""',  # Escape double quotes for CSV
        }

        cleaned = text
        for char, replacement in replacements.items():
            cleaned = cleaned.replace(char, replacement)

        # Remove excessive whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned)

        return cleaned.strip()

    def save_page_to_csv(self, page_data, csv_path, folder_name, page_number, section_name, author):
        """Save webpage data to CSV with ALL requested columns"""
        try:
            # Prepare CSV data with all requested columns
            csv_data = {
                'page_number': page_number,
                'folder_name': folder_name,
                'url': page_data.get('url', ''),
                'title': page_data.get('title', '')[:1000],
                'content': page_data.get('content', '')[:50000],  # Limit content length
                'author': author[:200],
                'word_count': page_data.get('word_count', 0)
            }

            # Define column order (EXACTLY as requested)
            fieldnames = [
                'page_number',
                'folder_name',
                'url',
                'title',
                'content',
                'author',
                'word_count'
            ]

            # Write to CSV
            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow(csv_data)

            return True

        except Exception as e:
            print(f"      âš ï¸ Error saving page CSV: {e}")
            return False

    def save_all_csvs(self, summary_data, successful, total, elapsed):
        """Save all CSV files"""
        print("\nğŸ’¾ Saving all CSV files...")

        # 1. Main summary CSV (with all columns)
        self.save_summary_csv(summary_data)

        # 2. URL mapping CSV
        self.save_url_mapping_csv(summary_data)

        # 3. Scraping summary CSV
        self.save_scraping_summary_csv(successful, total, elapsed)

        # 4. Section summary CSV
        self.save_section_summary_csv(summary_data)

        # 5. Errors CSV (if any)
        if self.errors:
            self.save_errors_csv()

    def save_summary_csv(self, data):
        """Save main summary CSV with all columns"""
        csv_path = os.path.join(self.base_dir, "all_pages_summary.csv")
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'page_number',
                    'folder_name',
                    'url',
                    'title',
                    'content',
                    'author',
                    'word_count',
                    'section'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            print(f"âœ… Main summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving main summary CSV: {e}")
            return False

    def save_url_mapping_csv(self, summary_data):
        """Save URL mapping CSV"""
        csv_path = os.path.join(self.base_dir, "url_mapping.csv")
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['folder_name', 'url', 'title', 'section', 'page_number']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort by section and page_number
                sorted_data = sorted(summary_data,
                                     key=lambda x: (x['section'], x['page_number']))

                for item in sorted_data:
                    writer.writerow({
                        'folder_name': item['folder_name'],
                        'url': item['url'],
                        'title': item['title'],
                        'section': item['section'],
                        'page_number': item['page_number']
                    })

            print(f"âœ… URL mapping CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving URL mapping CSV: {e}")
            return False

    def save_scraping_summary_csv(self, successful, total, elapsed):
        """Save scraping summary CSV"""
        csv_path = os.path.join(self.base_dir, "scraping_summary.csv")
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'total_pages',
                    'successful',
                    'failed',
                    'success_rate',
                    'time_elapsed_minutes',
                    'pages_per_minute',
                    'workers_used',
                    'date_completed'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow({
                    'total_pages': total,
                    'successful': successful,
                    'failed': len(self.errors),
                    'success_rate': f"{(successful / total * 100):.1f}%",
                    'time_elapsed_minutes': f"{int(elapsed / 60)}m {int(elapsed % 60)}s",
                    'pages_per_minute': f"{(total / elapsed * 60):.1f}",
                    'workers_used': self.max_workers,
                    'date_completed': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })

            print(f"âœ… Scraping summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving scraping summary CSV: {e}")
            return False

    def save_section_summary_csv(self, summary_data):
        """Save section summary CSV"""
        csv_path = os.path.join(self.base_dir, "section_summary.csv")
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            section_stats = {}

            # Calculate section statistics
            for item in summary_data:
                section = item['section']
                if section not in section_stats:
                    section_stats[section] = {'total': 0, 'success': 0, 'words': []}

                section_stats[section]['total'] += 1
                if item['word_count'] > 0:
                    section_stats[section]['success'] += 1
                    section_stats[section]['words'].append(item['word_count'])

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'total_pages', 'successful', 'failed', 'success_rate', 'avg_words']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort sections numerically
                sorted_sections = sorted(section_stats.items(),
                                         key=lambda x: int(x[0].split('_')[0]))

                for section, stats in sorted_sections:
                    avg_words = sum(stats['words']) / len(stats['words']) if stats['words'] else 0
                    failed = stats['total'] - stats['success']
                    writer.writerow({
                        'section': section,
                        'total_pages': stats['total'],
                        'successful': stats['success'],
                        'failed': failed,
                        'success_rate': f"{(stats['success'] / stats['total'] * 100):.1f}%" if stats[
                                                                                                   'total'] > 0 else "0%",
                        'avg_words': f"{avg_words:.0f}"
                    })

            print(f"âœ… Section summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving section summary CSV: {e}")
            return False

    def save_errors_csv(self):
        """Save errors CSV"""
        csv_path = os.path.join(self.base_dir, "scraping_errors.csv")
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'page_number', 'url', 'folder_name', 'error']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort errors by section and page number
                sorted_errors = sorted(self.errors,
                                       key=lambda x: (x['section'], x.get('position', 0)))

                for err in sorted_errors:
                    writer.writerow({
                        'section': err['section'],
                        'page_number': err.get('position', 0),
                        'url': err['url'],
                        'folder_name': f"{err['section'].split('_')[0]}_{err.get('position', 0):03d}_error",
                        'error': err['error'][:500]
                    })

            print(f"âœ… Errors CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving errors CSV: {e}")
            return False

    def create_zip_file(self):
        """Create a zip file of all scraped evidence in order"""
        try:
            zip_path = "gavin_menzies_scraped.zip"

            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Walk through directories in proper order
                website_structure = self.get_website_structure()

                for section_name in website_structure.keys():
                    section_path = os.path.join(self.base_dir, section_name)
                    if os.path.exists(section_path):
                        # Get all folders and sort them by position number
                        folder_items = []
                        for item in os.listdir(section_path):
                            item_path = os.path.join(section_path, item)
                            if os.path.isdir(item_path):
                                # Extract position from folder name
                                match = re.search(r'_(\d{3})_', item)
                                if match:
                                    position = int(match.group(1))
                                    folder_items.append((position, item, item_path))
                                else:
                                    # If no position found, try to get from the name
                                    pos_match = re.search(r'(\d{3})',
                                                          item.split('_')[1] if len(item.split('_')) > 1 else '000')
                                    position = int(pos_match.group(1)) if pos_match else 999
                                    folder_items.append((position, item, item_path))

                        # Sort by position
                        folder_items.sort(key=lambda x: x[0])

                        # Add to zip in sorted order
                        for position, folder_name, folder_path in folder_items:
                            for root, dirs, files in os.walk(folder_path):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    arcname = os.path.relpath(file_path, ".")
                                    zipf.write(file_path, arcname)

                # Add CSV files from base directory
                base_csv_dir = self.base_dir
                if os.path.exists(base_csv_dir):
                    for file in os.listdir(base_csv_dir):
                        if file.endswith('.csv') or file.endswith('.json') or file.endswith('.txt'):
                            file_path = os.path.join(base_csv_dir, file)
                            arcname = os.path.relpath(file_path, ".")
                            zipf.write(file_path, arcname)

            print(f"âœ… Zip file created with proper ordering: {zip_path}")
            return zip_path

        except Exception as e:
            print(f"âš ï¸ Could not create zip file: {e}")
            return None


def main():
    """Run the website scraper"""
    print("=" * 70)
    print("ğŸŒ GAVIN MENZIES WEBSITE SCRAPER - THREADED WITH ORDER PRESERVATION")
    print("   âœ“ Threaded scraping (5x faster)")
    print("   âœ“ Pages processed IN ORDER within each section")
    print("   âœ“ CSV files with ALL requested columns including content and author")
    print("   âœ“ Proper folder ordering (001, 002, 003, etc.)")
    print("=" * 70)

    print("\nğŸ“Š CSV COLUMNS FOR EACH PAGE (EXACT AS REQUESTED):")
    print("   page_number  - Sequential number in section (1, 2, 3, etc.)")
    print("   folder_name  - Readable folder name (01_001_gavin_menzies_home)")
    print("   url          - Page URL")
    print("   title        - Page title")
    print("   content      - Main text content")
    print("   author       - Author name (if available)")
    print("   word_count   - Number of words in content")

    print("\nğŸ“ OUTPUT STRUCTURE (PROPERLY ORDERED):")
    print("   gavin_menzies_scraped/")
    print("   â”œâ”€â”€ 01_main_website/")
    print("   â”‚   â”œâ”€â”€ 01_001_gavin_menzies_home_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/01_001_gavin_menzies_home_hash.csv")
    print("   â”‚   â”œâ”€â”€ 01_002_blog_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/01_002_blog_hash.csv")
    print("   â”‚   â”œâ”€â”€ 01_003_books_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/01_003_books_hash.csv")
    print("   â”‚   â””â”€â”€ ... (pages 004-009 in correct order)")
    print("   â”œâ”€â”€ 02_america_book/")
    print("   â”‚   â”œâ”€â”€ 02_001_who_discovered_america_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/02_001_who_discovered_america_hash.csv")
    print("   â”‚   â”œâ”€â”€ 02_002_extract_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/02_002_extract_hash.csv")
    print("   â”‚   â””â”€â”€ 02_003_the_book_hash/")
    print("   â”‚       â””â”€â”€ data/02_003_the_book_hash.csv")
    print("   â”œâ”€â”€ all_pages_summary.csv")
    print("   â”œâ”€â”€ url_mapping.csv")
    print("   â”œâ”€â”€ scraping_summary.csv")
    print("   â””â”€â”€ ... (other summary files)")

    print(f"\nğŸ“Š Total sections: 7")
    print(f"ğŸ“Š Total website pages: 57")

    # Ask for number of workers
    try:
        workers_input = input(f"\nEnter number of threads per section (1-10, default 5): ").strip()
        if workers_input:
            max_workers = min(max(1, int(workers_input)), 10)
        else:
            max_workers = 5
    except:
        max_workers = 5

    print(f"\nâš ï¸  WARNING: This will scrape 57 pages from gavinmenzies.net!")
    print(f"   Using {max_workers} threads per section")
    print("   Sections processed sequentially, URLs within sections in parallel")
    print("   Estimated time: 3-6 minutes (vs 30-60 minutes single-threaded)")
    print("   Estimated storage: ~50-100MB")
    print("   Folders numbered 001, 002, 003 for perfect ordering")

    proceed = input("\nProceed with scraping? (y/n): ").strip().lower()
    if proceed != 'y':
        print("Scraping cancelled.")
        exit(0)

    print("\n" + "=" * 70)
    print(f"ğŸš€ Starting Gavin Menzies website scraping with {max_workers} threads...")
    print("â³ Sections processed sequentially, URLs within sections in parallel.")
    print("ğŸ“ Folders created in correct numerical order (001, 002, 003).")
    print("=" * 70)

    scraper = GavinMenziesScraper(max_workers=max_workers)
    success_count, zip_file = scraper.scrape_all_pages()

    print("\nğŸ“Š FINAL SUMMARY")
    print(f"âœ… Successfully scraped: {success_count} pages")
    print(f"ğŸ“‚ Output folder: gavin_menzies_scraped/")

    if zip_file:
        print(f"ğŸ“¦ Zip file: {zip_file}")

    print("\nğŸ“„ Output CSV files (ALL IN CORRECT ORDER):")
    print("   - all_pages_summary.csv (complete dataset with all columns)")
    print("   - url_mapping.csv (folder to URL mapping with sections/positions)")
    print("   - scraping_summary.csv (statistics with performance metrics)")
    print("   - section_summary.csv (per-section stats with averages)")

    if scraper.errors:
        print("   - scraping_errors.csv (error details, sorted by section/position)")
        print(f"\nâš ï¸  {len(scraper.errors)} pages failed to scrape")

    print("\nâœ¨ Example folder structure (all in correct order):")
    print("   Section 01_main_website:")
    print("   01_001_gavin_menzies_home_hash")
    print("   01_002_blog_hash")
    print("   01_003_books_hash")
    print("   01_004_contact_hash")
    print("   01_005_1421_foundation_hash")
    print("   ... (continues 006, 007, 008, 009)")
    print("\n   Section 02_america_book:")
    print("   02_001_who_discovered_america_hash")
    print("   02_002_extract_hash")
    print("   02_003_the_book_hash")

    print("\nâš¡ Threaded scraping complete!")
    print("   All pages scraped with complete content and perfect ordering.")


if __name__ == "__main__":
    main()
