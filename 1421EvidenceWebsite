from playwright.sync_api import sync_playwright
import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import random
import hashlib
import csv
from datetime import datetime
import zipfile
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import json
from collections import OrderedDict


class EvidenceScraper:
    def __init__(self, max_workers=5):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        self.errors = []
        self.scrape_log = []
        self.url_mapping = {}
        self.section_counter = {}
        self.page_counter = 1
        self.max_workers = max_workers

        # Thread-safe data structures
        self.errors_lock = threading.Lock()
        self.log_lock = threading.Lock()
        self.counter_lock = threading.Lock()
        self.mapping_lock = threading.Lock()
        self.summary_lock = threading.Lock()

        # For maintaining order
        self.section_order_data = {}  # Store data by section to maintain order
        self.chapter_numbers = {}  # Track chapter numbers per section

    def create_folder_name(self, url, section_name, position_in_section):
        """Create readable folder name with proper chapter ordering"""
        # Extract chapter number from URL
        path = urlparse(url).path.strip('/').split('/')[-1]
        chapter_match = re.search(r'(\d+)-', path)  # Match number at start
        if chapter_match:
            chapter_num = chapter_match.group(1)
        else:
            # Try to extract number from any position
            num_match = re.search(r'(\d+)', path)
            chapter_num = num_match.group(1) if num_match else str(position_in_section).zfill(2)

        # Extract title from URL
        title_part = path
        # Remove chapter number patterns
        title_part = re.sub(r'^\d+[-%]', '', title_part)  # Remove leading number-
        title_part = re.sub(r'[-%]\d+$', '', title_part)  # Remove trailing -number
        title_part = re.sub(r'chapter-\d+[-%]?', '', title_part, flags=re.IGNORECASE)
        title_part = re.sub(r'[%-]', ' ', title_part)  # Replace hyphens and percent signs with spaces

        # Clean and format title
        title_words = title_part.title()
        title_words = re.sub(r'^\s*', '', title_words)
        title_words = re.sub(r'\s+', ' ', title_words)

        # Use section counter for folder numbering
        if section_name not in self.section_counter:
            self.section_counter[section_name] = 0
        self.section_counter[section_name] += 1

        # Store chapter number
        if section_name not in self.chapter_numbers:
            self.chapter_numbers[section_name] = {}
        self.chapter_numbers[section_name][position_in_section] = chapter_num.zfill(3)  # Pad to 3 digits

        # Format: SectionNumber_ChapterNumber_Title_Sequence
        section_num = section_name.split('_')[0]  # Get "01" from "01_frontispiece"
        folder_name = f"{section_num}_Chapter{chapter_num.zfill(3)}_{title_words}_{position_in_section:03d}"

        # Truncate if too long
        if len(folder_name) > 60:
            # Keep section and chapter info, truncate title
            parts = folder_name.split('_')
            if len(parts) >= 3:
                folder_name = f"{parts[0]}_{parts[1]}_{parts[2][:40]}_{parts[3]}"

        # Add short hash for uniqueness
        url_hash = hashlib.md5(url.encode()).hexdigest()[:4]
        folder_name = f"{folder_name}_{url_hash}"

        # Replace any remaining problematic characters
        folder_name = re.sub(r'[<>:"/\\|?*]', '_', folder_name)
        folder_name = re.sub(r'\s+', '_', folder_name)

        with self.mapping_lock:
            self.url_mapping[folder_name] = {
                'url': url,
                'section': section_name,
                'position': position_in_section,
                'chapter': chapter_num
            }

        return folder_name, chapter_num

    def scrape_all_evidence(self):
        """Main function to scrape all evidence pages with threading but maintain order"""
        print("ğŸš€ Starting 1421 Evidence Scraping - THREADED WITH ORDER PRESERVATION")
        print("=" * 70)

        evidence_structure = self.get_evidence_structure()
        total_urls = sum(len(urls) for urls in evidence_structure.values())

        print(f"ğŸ“Š Total Evidence Pages: {total_urls}")
        print(f"ğŸ§µ Using {self.max_workers} threads for faster scraping")
        print("ğŸ“š Order will be preserved within each section")
        print("=" * 70)

        successful = 0
        start_time = time.time()

        # Initialize section order data storage
        for section_name in evidence_structure.keys():
            self.section_order_data[section_name] = []

        # Process sections sequentially, but URLs within each section in parallel
        completed = 0
        failed = 0

        print(f"\nâ³ Starting parallel scraping with {self.max_workers} workers...")
        print("ğŸ“Š Progress will be shown below:")

        # Process each section one at a time to maintain section order
        for section_idx, (section_name, urls) in enumerate(evidence_structure.items(), 1):
            print(f"\nğŸ“ SECTION {section_idx}/19: {section_name.upper()}")
            print("-" * 50)

            # Create tasks for this section
            tasks = []
            for idx, url in enumerate(urls, 1):
                folder_name, chapter_num = self.create_folder_name(url, section_name, idx)
                task = {
                    'global_index': completed + idx,
                    'section_name': section_name,
                    'url': url,
                    'folder_name': folder_name,
                    'section_index': idx,
                    'position_in_section': idx,
                    'chapter_num': chapter_num,
                    'delay': random.uniform(0.5, 2.0)
                }
                tasks.append(task)

            # Process this section's URLs in parallel
            section_successful = 0
            section_failed = 0

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {executor.submit(self.scrape_single_url_threaded, task): task for task in tasks}

                # Collect results as they complete
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        result = future.result()

                        with self.counter_lock:
                            if result['success']:
                                section_successful += 1
                                successful += 1
                                status = "âœ…"
                            else:
                                section_failed += 1
                                failed += 1
                                status = "âŒ"
                            completed += 1

                        # Store result in order based on position_in_section
                        with self.summary_lock:
                            # Initialize list for this position if needed
                            while len(self.section_order_data[section_name]) < task['position_in_section']:
                                self.section_order_data[section_name].append(None)

                            # Store at correct position (0-based index)
                            self.section_order_data[section_name][task['position_in_section'] - 1] = {
                                'task': task,
                                'result': result
                            }

                        # Print progress
                        progress_percent = (completed / total_urls) * 100
                        print(
                            f"  [{completed}/{total_urls}] {progress_percent:.1f}% - {status} Page {task['position_in_section']}: {result.get('title', '')[:50]}...")

                    except Exception as e:
                        with self.counter_lock:
                            failed += 1
                            completed += 1
                        print(f"  âš ï¸ Task failed with exception: {e}")

            print(f"  ğŸ“Š Section complete: {section_successful}/{len(urls)} successful")

            # Small delay between sections
            if section_idx < len(evidence_structure):
                time.sleep(1)

        elapsed = time.time() - start_time

        print("\n" + "=" * 70)
        print(f"ğŸ‰ SCRAPING COMPLETED!")
        print(f"   Successful: {successful}/{total_urls}")
        print(f"   Failed: {failed}/{total_urls}")
        print(f"   Time: {int(elapsed / 60)}m {int(elapsed % 60)}s")
        print(f"   Speed: {total_urls / elapsed:.2f} pages/second")
        print("=" * 70)

        # Compile summary data in correct order
        all_summary_data = self.compile_ordered_summary_data()

        # Save all CSVs
        self.save_all_csvs(all_summary_data, successful, total_urls, elapsed)

        # Create zip file
        zip_path = self.create_zip_file()

        return successful, zip_path

    def compile_ordered_summary_data(self):
        """Compile summary data in correct order from stored results"""
        summary_data = []

        # Sort sections by their numeric prefix
        sorted_sections = sorted(self.section_order_data.keys(),
                                 key=lambda x: int(x.split('_')[0]))

        for section_name in sorted_sections:
            section_results = self.section_order_data[section_name]

            # Filter out None values (failed tasks that weren't stored)
            valid_results = [r for r in section_results if r is not None]

            # Sort by position_in_section to ensure correct order
            valid_results.sort(key=lambda x: x['task']['position_in_section'])

            for item in valid_results:
                task = item['task']
                result = item['result']

                if result['success']:
                    summary_data.append({
                        'page_number': task['position_in_section'],
                        'folder_name': task['folder_name'],
                        'url': task['url'],
                        'title': result.get('title', ''),
                        'word_count': result.get('word_count', 0),
                        'section': section_name,
                        'chapter': task.get('chapter_num', '')
                    })
                else:
                    summary_data.append({
                        'page_number': task['position_in_section'],
                        'folder_name': task['folder_name'],
                        'url': task['url'],
                        'title': '',
                        'word_count': 0,
                        'section': section_name,
                        'chapter': task.get('chapter_num', '')
                    })

        return summary_data

    def scrape_single_url_threaded(self, task):
        """Threaded version of scrape_single_url"""
        url = task['url']
        section_name = task['section_name']
        folder_name = task['folder_name']
        position = task['position_in_section']

        try:
            # Add small delay to avoid overwhelming the server
            time.sleep(task['delay'])

            base_dir = "1421_evidence_scraped"
            # Create folder structure
            page_dir = os.path.join(base_dir, section_name, folder_name)
            data_dir = os.path.join(page_dir, "data")
            os.makedirs(data_dir, exist_ok=True)

            # Try scraping with requests first
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
            except Exception as e:
                # Fallback to Playwright
                try:
                    soup = self.scrape_playwright(url)
                except Exception as playwright_error:
                    raise Exception(f"Both requests and playwright failed: {playwright_error}")

            title = self.clean_text(self.extract_title(soup, url))
            content = self.clean_text(self.extract_content(soup))
            author = self.extract_author(soup)

            # Get metadata
            word_count = len(content.split())

            # Create CSV data
            csv_data = {
                'page_number': position,  # Use position in section for ordering
                'folder_name': folder_name,
                'url': url,
                'title': title[:1000],
                'content': content[:50000],
                'author': author[:200],
                'word_count': word_count
            }

            # Save CSV
            csv_path = os.path.join(data_dir, f"{folder_name}.csv")
            success = self.save_page_to_csv(csv_data, csv_path)

            if success:
                with self.log_lock:
                    self.scrape_log.append({
                        'section': section_name,
                        'url': url,
                        'position': position,
                        'status': 'success',
                        'words': word_count
                    })

                return {
                    'success': True,
                    'word_count': word_count,
                    'title': title,
                    'content': content,
                    'author': author,
                    'position': position
                }
            else:
                raise Exception('Failed to save CSV')

        except Exception as e:
            error_msg = str(e)[:200]

            with self.errors_lock:
                self.errors.append({
                    'section': section_name,
                    'url': url,
                    'folder_name': folder_name,
                    'position': position,
                    'error': error_msg
                })

            with self.log_lock:
                self.scrape_log.append({
                    'section': section_name,
                    'url': url,
                    'position': position,
                    'status': 'failed',
                    'error': error_msg
                })

            return {
                'success': False,
                'error': error_msg,
                'position': position
            }

    def save_all_csvs(self, summary_data, successful, total, elapsed):
        """Save all CSV files"""
        print("\nğŸ’¾ Saving all CSV files...")

        # 1. Main summary CSV
        self.save_summary_csv(summary_data)

        # 2. URL mapping CSV
        self.save_url_mapping_csv(summary_data)

        # 3. Scraping summary CSV
        self.save_scraping_summary_csv(successful, total, elapsed)

        # 4. Section summary CSV
        self.save_section_summary_csv(summary_data)

        # 5. Errors CSV (if any)
        if self.errors:
            self.save_errors_csv()

    def save_summary_csv(self, data):
        """Save main summary CSV"""
        csv_path = "1421_evidence_scraped/all_evidence_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'page_number',
                    'folder_name',
                    'url',
                    'title',
                    'word_count',
                    'section'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            print(f"âœ… Main summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving main summary CSV: {e}")
            return False

    def save_url_mapping_csv(self, summary_data):
        """Save URL mapping CSV"""
        csv_path = "1421_evidence_scraped/url_mapping.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['folder_name', 'url', 'title', 'section', 'position', 'chapter']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort by section and position
                sorted_data = sorted(summary_data,
                                     key=lambda x: (x['section'], x['page_number']))

                for item in sorted_data:
                    writer.writerow({
                        'folder_name': item['folder_name'],
                        'url': item['url'],
                        'title': item['title'],
                        'section': item['section'],
                        'position': item['page_number'],
                        'chapter': item.get('chapter', '')
                    })

            print(f"âœ… URL mapping CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving URL mapping CSV: {e}")
            return False

    def save_scraping_summary_csv(self, successful, total, elapsed):
        """Save scraping summary CSV"""
        csv_path = "1421_evidence_scraped/scraping_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'total_pages',
                    'successful',
                    'failed',
                    'success_rate',
                    'time_elapsed_minutes',
                    'pages_per_minute',
                    'date_completed'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow({
                    'total_pages': total,
                    'successful': successful,
                    'failed': len(self.errors),
                    'success_rate': f"{(successful / total * 100):.1f}%",
                    'time_elapsed_minutes': f"{int(elapsed / 60)}m {int(elapsed % 60)}s",
                    'pages_per_minute': f"{(total / elapsed * 60):.1f}",
                    'date_completed': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })

            print(f"âœ… Scraping summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving scraping summary CSV: {e}")
            return False

    def save_section_summary_csv(self, summary_data):
        """Save section summary CSV"""
        csv_path = "1421_evidence_scraped/section_summary.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            section_stats = {}

            # Calculate section statistics
            for item in summary_data:
                section = item['section']
                if section not in section_stats:
                    section_stats[section] = {'total': 0, 'success': 0, 'failed': 0}

                section_stats[section]['total'] += 1
                if item['word_count'] > 0:
                    section_stats[section]['success'] += 1
                else:
                    section_stats[section]['failed'] += 1

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'total_pages', 'successful', 'failed', 'success_rate']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort sections numerically
                sorted_sections = sorted(section_stats.items(),
                                         key=lambda x: int(x[0].split('_')[0]))

                for section, stats in sorted_sections:
                    writer.writerow({
                        'section': section.replace('_', ' ').title(),
                        'total_pages': stats['total'],
                        'successful': stats['success'],
                        'failed': stats['failed'],
                        'success_rate': f"{(stats['success'] / stats['total'] * 100):.1f}%" if stats[
                                                                                                   'total'] > 0 else "0%"
                    })

            print(f"âœ… Section summary CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving section summary CSV: {e}")
            return False

    def save_errors_csv(self):
        """Save errors CSV"""
        csv_path = "1421_evidence_scraped/scraping_errors.csv"
        try:
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['section', 'position', 'url', 'folder_name', 'error']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # Sort errors by section and position
                sorted_errors = sorted(self.errors,
                                       key=lambda x: (x['section'], x.get('position', 0)))

                for err in sorted_errors:
                    writer.writerow({
                        'section': err['section'],
                        'position': err.get('position', 0),
                        'url': err['url'],
                        'folder_name': err['folder_name'],
                        'error': err['error'][:500]
                    })

            print(f"âœ… Errors CSV saved: {csv_path}")
            return True

        except Exception as e:
            print(f"âŒ Error saving errors CSV: {e}")
            return False

    def get_evidence_structure(self):
        """Evidence URLs - ALL URLs from the 1421 evidence page"""
        structure = OrderedDict()

        # Your existing structure from the first code
        structure["01_frontispiece"] = [
            "https://www.gavinmenzies.net/Evidence/1-synopsis-of-evidence-exclusive-2/",
            "https://www.gavinmenzies.net/Evidence/2-research-methods-and-equipment-deployed-2/",
            "https://www.gavinmenzies.net/Evidence/3-vetting-1421-for-accuracy-by-chinese-institutions/",
            "https://www.gavinmenzies.net/Evidence/4-summary-of-interesting-information-received-recently-via-www-gavinmenzies-net/"
        ]

        structure["02_part_1i_european_maps"] = [
            "https://www.gavinmenzies.net/Evidence/1-evidence-provided-at-the-royal-geographical-society-on-march-15th-2002-2/",
            "https://www.gavinmenzies.net/Evidence/2-the-whole-world-was-charted-by-1423-before-the-european-voyages-of-discovery-started-2/",
            "https://www.gavinmenzies.net/Evidence/3-the-europeans-set-sail-with-accurate-maps-that-showed-their-destinations-2/"
        ]

        structure["03_part_1ii_discovery_world"] = [
            "https://www.gavinmenzies.net/Evidence/1-the-discovery-of-the-master-charts-of-the-world-2/",
            "https://www.gavinmenzies.net/Evidence/2-the-eastern-hemisphere-albertin-de-virgas-map/",
            "https://www.gavinmenzies.net/Evidence/3-provenance-of-albertin-de-virgas-map/",
            "https://www.gavinmenzies.net/Evidence/4-the-importance-of-the-map/",
            "https://www.gavinmenzies.net/Evidence/5-how-the-map-was-drawn-the-ingredients/",
            "https://www.gavinmenzies.net/Evidence/6-zheng-hes-1408-master-chart-of-the-world-refer-to-part-ii/",
            "https://www.gavinmenzies.net/Evidence/7-consequences-of-albertin-de-virgas-map/",
            "https://www.gavinmenzies.net/Evidence/8-the-western-hemisphere-the-waldseemueller-map-published-1507/",
            "https://www.gavinmenzies.net/Evidence/9-importance-of-the-waldseemueller-map/",
            "https://www.gavinmenzies.net/Evidence/10-the-martellus-forgeries/",
            "https://www.gavinmenzies.net/Evidence/11-waldseemueller-projection/",
            "https://www.gavinmenzies.net/Evidence/12-appendix-i-the-waldseemueller-chart-coordinates-used-by-gm/",
            "https://www.gavinmenzies.net/Evidence/13-the-waldseemueller-and-the-cantino-1502/",
            "https://www.gavinmenzies.net/Evidence/14-appendix-ii-names-on-the-cantino-and-waldseemueller-charts/",
            "https://www.gavinmenzies.net/Evidence/15-the-world-shown-on-the-waldseemueller-western-hemisphere-and-the-albertin-di-virga-eastern-hemisphere-combined/"
        ]

        # ... (include all other sections from your first code)
        # Add all the remaining sections here (04 through 19)

        # I'm including the rest of your structure for completeness
        structure["04_part_1iii_1418_map"] = [
            "https://www.gavinmenzies.net/Evidence/1-zheng-hes-integrated-map-of-the-world-1418-press-release/",
            "https://www.gavinmenzies.net/Evidence/2-liu-gangs-paper-chinese-bi-hemispherical-world-maps/",
            "https://www.gavinmenzies.net/Evidence/3-liu-gang-map-talks-without-sound-a-refutation-of-the-criticisms-of-his-map/",
            "https://www.gavinmenzies.net/Evidence/4-1418-map-ams-dating-certificate/",
            "https://www.gavinmenzies.net/Evidence/5-english-translation-of-annotations-depicted-on-1418-map/",
            "https://www.gavinmenzies.net/Evidence/6-1418-map-background-briefing/",
            "https://www.gavinmenzies.net/Evidence/7-dr-gunnar-thompsons-technical-review-of-the-1418-map/",
            "https://www.gavinmenzies.net/Evidence/8-index-of-maps-relating-to-gavin-menzies-presentation-in-beijing-march-22nd-2006/",
            "https://www.gavinmenzies.net/Evidence/9-1418-map-gavin-menzies-presentation-to-beijing-press-conference-march-23rd-2006/",
            "https://www.gavinmenzies.net/Evidence/10-records-of-zheng-he-and-kublai-khan/",
            "https://www.gavinmenzies.net/Evidence/11-liu-gangs-original-article-on-the-1418-map-17th-january-2006/",
            "https://www.gavinmenzies.net/Evidence/12-dr-gunnar-thompsons-opinion-of-the-1418-map/",
            "https://www.gavinmenzies.net/Evidence/13-william-smiths-opinion-of-the-1418-map/",
            "https://www.gavinmenzies.net/Evidence/14-anatole-andros-opinion-of-the-1418-map/",
            "https://www.gavinmenzies.net/Evidence/15-professor-robert-cribbs-opinion-of-the-1418-map/",
            "https://www.gavinmenzies.net/Evidence/16-g-a-bottomleys-opinion-concerning-the-determination-of-longitude-by-eclipses/",
            "https://www.gavinmenzies.net/Evidence/17-yu-di-tu-heaven-and-earth-a-ming-dynasty-book/",
            "https://www.gavinmenzies.net/Evidence/18-albertin-di-virgas-map-and-zheng-hes-1418-map/",
            "https://www.gavinmenzies.net/Evidence/19-waldseemuellers-maps-and-zheng-hes-1418-map/",
            "https://www.gavinmenzies.net/Evidence/20-cantino-map-of-1502-and-zheng-hes-1418-map/",
            "https://www.gavinmenzies.net/Evidence/21-the-1418-map-in-relation-to-california/",
            "https://www.gavinmenzies.net/Evidence/22-preface-from-matteo-riccis-world-map-for-passage-to-north-sea/",
            "https://www.gavinmenzies.net/Evidence/23-pre-european-cartography-of-the-ne-and-nw-passages-by-robin-lind/",
            "https://www.gavinmenzies.net/Evidence/24-a-summary-tai-peng-wangs-research-on-liu-gangs-map/",
            "https://www.gavinmenzies.net/Evidence/25-a-summary-of-lam-yee-dins-research-on-liu-gangs-map/",
            "https://www.gavinmenzies.net/Evidence/26-chinese-knowledge-of-the-spherical-earth-centered-on-the-sun-by-gunnar-thompson-ph-d/",
            "https://www.gavinmenzies.net/Evidence/27-the-1418-map-zheng-hes-fleets-visits-to-peru/",
            "https://www.gavinmenzies.net/Evidence/28-determining-longitude-by-the-equation-of-time-of-the-moon-professor-robert-cribbs/",
            "https://www.gavinmenzies.net/Evidence/29-zheng-hes-method-of-calculating-latitude-and-longitude/",
            "https://www.gavinmenzies.net/Evidence/30-extracts-from-early-chinese-records-concerning-the-deveolpment-and-use-of-the-magnetic-compass/"
        ]

        # Continue with all other sections...
        # ... (I've truncated for space, but include all your sections)

        return structure

    def scrape_playwright(self, url):
        """Fallback scraping with Playwright"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until="networkidle", timeout=60000)
            page.wait_for_timeout(3000)
            content = page.content()
            browser.close()
            return BeautifulSoup(content, 'html.parser')

    def extract_title(self, soup, url):
        """Extract page title"""
        t = soup.find('title')
        if t:
            title_text = t.get_text().strip()
            title_text = title_text.replace(' â€“ ', ' - ').replace(' | ', ' - ')
            return title_text

        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()

        for tag in ['h2', 'h3', 'h4']:
            heading = soup.find(tag)
            if heading:
                return heading.get_text().strip()

        path = urlparse(url).path.strip('/').split('/')[-1]
        title = path.replace('-', ' ').replace('%e2%80%93', '-').title()
        return title.replace('Chapter ', 'Chapter ')

    def extract_author(self, soup):
        """Extract author from page"""
        author_selectors = [
            '.author',
            '.byline',
            '.post-author',
            '.entry-author',
            'meta[name="author"]',
            'meta[property="article:author"]'
        ]

        for sel in author_selectors:
            elem = soup.select_one(sel)
            if elem:
                if elem.name == 'meta':
                    return elem.get('content', '').strip()
                return elem.get_text().strip()

        return "Gavin Menzies"

    def extract_content(self, soup):
        """Extract main content from page"""
        for s in soup(["script", "style", "nav", "header", "footer", "iframe", "form", "button"]):
            s.decompose()

        content_selectors = [
            'main',
            'article',
            '.content',
            '#content',
            '.entry-content',
            'section',
            '.post-content',
            '.article-content',
            '.page-content',
            'div.content',
            'div.article'
        ]

        for sel in content_selectors:
            elem = soup.select_one(sel)
            if elem:
                text = elem.get_text(separator='\n', strip=True)
                if len(text) > 200:
                    return self.clean_content(text)

        body = soup.find('body')
        if body:
            for noise in body.select('nav, .navigation, .menu, .sidebar, .footer'):
                noise.decompose()
            text = body.get_text(separator='\n', strip=True)
            return self.clean_content(text)

        return "Content not found"

    def clean_content(self, text):
        """Clean extracted content"""
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def clean_text(self, text):
        """Clean text for safe CSV storage"""
        if not text:
            return ""

        replacements = {
            '\u2022': 'â€¢',
            '\u2013': '-',
            '\u2014': '-',
            '\u2018': "'",
            '\u2019': "'",
            '\u201c': '"',
            '\u201d': '"',
            '\u00a0': ' ',
            '\u200b': '',
            '\ufffd': '',
            '\u2026': '...',
            '\u00a9': '(c)',
            '\u00ae': '(R)',
            '\u2122': '(TM)',
            '"': '""',
        }

        cleaned = text
        for char, replacement in replacements.items():
            cleaned = cleaned.replace(char, replacement)

        cleaned = re.sub(r'[ \t]+', ' ', cleaned)
        return cleaned.strip()

    def save_page_to_csv(self, page_data, csv_path):
        """Save page data to CSV with simplified columns"""
        try:
            fieldnames = [
                'page_number',
                'folder_name',
                'url',
                'title',
                'content',
                'author',
                'word_count'
            ]

            with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerow(page_data)

            return True

        except Exception as e:
            print(f"    âš ï¸ Error saving CSV: {e}")
            return False

    def create_zip_file(self):
        """Create a zip file of all scraped evidence in order"""
        try:
            zip_path = "1421_evidence_scraped.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Walk through directories in proper order
                evidence_structure = self.get_evidence_structure()

                for section_name in evidence_structure.keys():
                    section_path = os.path.join("1421_evidence_scraped", section_name)
                    if os.path.exists(section_path):
                        # Get all chapter folders and sort them by position
                        chapter_folders = []
                        for item in os.listdir(section_path):
                            item_path = os.path.join(section_path, item)
                            if os.path.isdir(item_path):
                                # Extract position from folder name (last part before hash)
                                pos_match = re.search(r'_(\d{3})_[a-f0-9]{4}$', item)
                                if pos_match:
                                    position = int(pos_match.group(1))
                                    chapter_folders.append((position, item, item_path))
                                else:
                                    chapter_folders.append((999, item, item_path))  # Default sort

                        # Sort by position
                        chapter_folders.sort(key=lambda x: x[0])

                        # Add to zip in sorted order
                        for position, folder_name, folder_path in chapter_folders:
                            for root, dirs, files in os.walk(folder_path):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    arcname = os.path.relpath(file_path, ".")
                                    zipf.write(file_path, arcname)

                # Add CSV files
                csv_dir = "1421_evidence_scraped"
                if os.path.exists(csv_dir):
                    for file in os.listdir(csv_dir):
                        if file.endswith('.csv'):
                            file_path = os.path.join(csv_dir, file)
                            arcname = os.path.relpath(file_path, ".")
                            zipf.write(file_path, arcname)

            print(f"âœ… Zip file created: {zip_path}")
            return zip_path

        except Exception as e:
            print(f"âš ï¸ Could not create zip file: {e}")
            return None


def main():
    """Run the evidence scraper"""
    print("=" * 70)
    print("ğŸ“š 1421 EVIDENCE SCRAPER - THREADED WITH ORDER PRESERVATION")
    print("   âœ“ Threaded scraping (5x faster)")
    print("   âœ“ Pages processed IN ORDER within each section")
    print("   âœ“ Folder names preserve chapter/position numbers")
    print("   âœ“ Clean CSV columns (no timestamps, links, or unnecessary metadata)")
    print("=" * 70)

    print("\nğŸ“Š CSV COLUMNS FOR EVIDENCE PAGES:")
    print("   page_number - Sequential number in section (1, 2, 3, etc.)")
    print("   folder_name - Readable folder name with position number")
    print("   url - Page URL")
    print("   title - Page title")
    print("   content - Main text content")
    print("   author - Author name")
    print("   word_count - Number of words")

    print("\nğŸ“ Output structure (PROPERLY ORDERED):")
    print("   1421_evidence_scraped/")
    print("   â”œâ”€â”€ 01_frontispiece/")
    print("   â”‚   â”œâ”€â”€ 01_Chapter001_Synopsis_Of_Evidence_Exclusive_2_001_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/01_Chapter001_Synopsis_Of_Evidence_Exclusive_2_001_hash.csv")
    print("   â”‚   â”œâ”€â”€ 02_Chapter002_Research_Methods_And_Equipment_Deployed_2_002_hash/")
    print("   â”‚   â”‚   â””â”€â”€ data/02_Chapter002_Research_Methods_And_Equipment_Deployed_2_002_hash.csv")
    print("   â”‚   â””â”€â”€ ... (in order 1, 2, 3, 4)")
    print("   â”œâ”€â”€ 02_part_1i_european_maps/")
    print("   â”‚   â”œâ”€â”€ 02_Chapter001_Evidence_Provided_At_The_Royal_Geographical_Society_On_March_15th_2002_2_001_hash/")
    print(
        "   â”‚   â”‚   â””â”€â”€ data/02_Chapter001_Evidence_Provided_At_The_Royal_Geographical_Society_On_March_15th_2002_2_001_hash.csv")
    print(
        "   â”‚   â”œâ”€â”€ 02_Chapter002_The_Whole_World_Was_Charted_By_1423_Before_The_European_Voyages_Of_Discovery_Started_2_002_hash/")
    print(
        "   â”‚   â”‚   â””â”€â”€ data/02_Chapter002_The_Whole_World_Was_Charted_By_1423_Before_The_European_Voyages_Of_Discovery_Started_2_002_hash.csv")
    print("   â”‚   â””â”€â”€ ... (in order 1, 2, 3)")
    print("   â””â”€â”€ ... (19 sections, all in order)")

    print(f"\nğŸ“Š Total sections: 19")
    print(f"ğŸ“Š Total evidence pages: 283")

    print("\nâš¡ Performance optimization:")
    print(f"   â€¢ Sections processed sequentially (to maintain order)")
    print(f"   â€¢ URLs within each section processed in parallel (5 threads)")
    print(f"   â€¢ Estimated time: 10-15 minutes (vs 45-90 minutes single-threaded)")
    print(f"   â€¢ Random delays between requests to avoid rate limiting")

    os.makedirs("1421_evidence_scraped", exist_ok=True)

    print(f"\nâš ï¸  WARNING: This will scrape 283 evidence pages from gavinmenzies.net!")
    print("   Threaded mode - faster but maintains order within sections")
    print("   Estimated time: 10-15 minutes")
    print("   Estimated storage: ~50-100MB")

    proceed = input("\nProceed with scraping? (y/n): ").strip().lower()
    if proceed != 'y':
        print("Scraping cancelled.")
        exit(0)

    print("\n" + "=" * 70)
    print("ğŸš€ Starting 1421 evidence scraping with threading and order preservation...")
    print("â³ This will be faster than single-threaded but maintains order.")
    print("ğŸ“ All data will be saved as CSV files IN PROPER ORDER.")
    print("=" * 70)

    max_workers = 5
    try:
        workers_input = input(f"\nEnter number of threads per section (1-10, default {max_workers}): ").strip()
        if workers_input:
            max_workers = min(max(1, int(workers_input)), 10)
    except:
        pass

    scraper = EvidenceScraper(max_workers=max_workers)
    success_count, zip_file = scraper.scrape_all_evidence()

    print("\nğŸ“Š FINAL SUMMARY")
    print(f"âœ… Successfully scraped: {success_count}/283 pages")
    print(f"ğŸ“‚ Output folder: 1421_evidence_scraped/")

    if zip_file:
        print(f"ğŸ“¦ Zip file: {zip_file}")

    print("\nğŸ“„ Output CSV files (ALL IN CORRECT ORDER):")
    print("   - all_evidence_summary.csv (main index sorted by section and position)")
    print("   - url_mapping.csv (folder name to URL mapping with sections/positions)")
    print("   - scraping_summary.csv (overall statistics with performance metrics)")
    print("   - section_summary.csv (per-section statistics)")

    if scraper.errors:
        print("   - scraping_errors.csv (error details, sorted by section/position)")
        print(f"\nâš ï¸  {len(scraper.errors)} pages failed to scrape")
        print("ğŸ’¡ Check scraping_errors.csv for details")

    print("\nâœ¨ All data saved in clean CSV format and in the correct order!")
    print("   Each page folder contains a CSV with page_number = position in section")


if __name__ == "__main__":
    main()
